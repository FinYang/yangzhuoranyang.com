<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>ETC2410</title>
    <meta charset="utf-8" />
    <meta name="author" content="Fin" />
    <link href="ETC2410_slides_S1_2019_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="ETC2410_slides_S1_2019_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ETC2410
### Fin
### Semester 1 2019

---




###Week 1

* Stages of empirical analysis

1. Understanding the problem
2. Formulating an appropriate conceptual model to tackle the problem
3. Collecting appropriate data
4. Looking at the data (Descriptive Analytics)
5. Estimating the model, making inference, predictions and policy presciptions as appropriate
6. Evaluation, learning and improving each of the previous steps, and iterating until the problem is solved 

--

```
e.g.
What elements will affect earnings?

```

---

* Converting an economic model to a statistical model  

 - conomic theory describes the systematic part - `\(f (i)\)`
 - `\(e\)` is the nonsystematic, random error component that we know is present, but cannot be observed
 - Adding random errors converts our economic model into a statistical one that gives us a basis for statistical inference
 
--
* Experimental and Non-experimental data  

--

 __Non-experimental__ data are not accumulated through controlled experiments on individuals, firms, or segments of the economy.  
 (observational data, retrospective data)  
 
--
 
 __Experimental__ data are often collected in laboratory environments in the natural sciences, but they are much more difficult to obtain in the social sciences.  

--

```
What data we want to collect about wage?
```

---

---

* Predictive analytics (no causality) and Prescriptive analytics (causal)

--

__Predictive__ analytics: using some variables to predict a target variable without any requirement of causality

--

__Prescriptive__ analytics to measure the causal relationship between variables, to prescribe how to achieve a desired change in the target variable by manipulating the cause.

--

We can always use multiple regression for prediction.  
We can sometimes use multiple regression to tease out causal relationships

--

```
Which one is for analyzing wage?
```


---



* Cross-sectional, Time-series, Panel or Longitudinal data, Pooled
 
--

A __Cross-sectional__ data set consists of a sample of individuals, households, firms, cities, states, contries, or a variety of other units, _taken at a given point in time_.  

--

A __Time Series__ data set consists of observations on a variable or several variables _over time_.  

--

A __Pooled__ data set combine data (on multiple variable) from _different individuals_ over time.  

--

A __Panel or Longitudinal__ data set consists of a time series for _each_ cross-sectional member in the data set.

--

```
Which one is for wage related data?
```

---

* **What time series data can do, but cross-section cannot:**  

 - Forecast future values of a variable  
 - Estimate the dynamic causal effect of one variable `\(x\)` (estimate the causal effect on y, over several time periods, of a change in `\(x\)` today. e.g. tax on alcohol)  

--

1.  Forecast future values of a variable:  
eg. stock prices, consumer price index, gross domestic product, annual homicide rates one or several days /months /quarters / years ahead.  

--

2.  Estimate the dynamic causal effect of one variable x on another variable y:   
eg. estimate the effect on alcohol consumption of an increase in the tax on alcohol, both initially and subsequently as consumers adjust to the new tax.  

--

```
wage?
```

---


* **Cross-sectional data, time series data, panel data**  
 Panel data: can be used to address questions that cannot be adequately addressed using either cross-section or time series data.
 
--
 
 - **Univariate time series**: A time series data set consisting of observations on a single variable.  
 - **Multivariate time series**: A time series data set consisting of observations on several variables.  
 
--
 
* Notation  
`\(y_t\)` : the value of the time series in time period t.  
`\(T\)` : sample size  

---



* **Properties**



  1. Observations on time series data are ordered.
  
--
   
  2. Time series data is generally characterized by some form of temporal dependence.  
  
--
  
  Because of temporal dependence, it is implaisible to assume that the random variable `\(y_t\)` and `\(y_{t-1}\)` are i.i.d.  
      The strength of temporal dependence can differ between time series. When the dependence is strong the time series is called _persistent_.

---

* Notation  

cross section analysis:  

`$$y_i = \beta_0 +\beta_1 x_{i1} +\beta_2x_{i2} + \ldots +\beta_kx_{ik} +u_i\ , \ \  \ i = 1, 2,\ldots, n$$`

--

time series analysis:  

`$$y_t = \beta_0 +\beta_1 x_{t1} +\beta_2x_{t2} + \ldots +\beta_kx_{tk} +u_t\ , \ \  \ t = 1, 2,\ldots, n$$`

---

* A general statistical model can be written as: 

`$$y=f(x)+e$$`

Where y denotes the random variablewe want to predict or analyze; x is the variable that are related or can explain some behaviour of y; random error e accounts for the many factors that affect y that we have omitted from this simple model



&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-1-1.png" width="45%" height="45%" /&gt;&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-1-2.png" width="45%" height="45%" /&gt;

--

```
Write a general model form for wage.
```



---

* Simple Linear Regression  
A complete model for a random variable is a model of its probability distribution. We model the conditional distribution of y given x, a model for `\(E(y|x)\)`.  

`$$\begin{array}{c|c}
y &amp;x \\
\hline Dependent\ \ Variable &amp; Independent\ \ Variable\\ 
Response\ \ Variable &amp; Control\ \  Variable\\ 
Explained\ \ Variable &amp; Explanatory\ \  Variable\\ 
Predicted\ \  Variable &amp; Predictor\ \  Variable\\ 
Regressand &amp; Regressor\\ 
\end{array}$$`


* Population and Sample format
    + Population 
        - `\(E(y|x)=\beta_0 + \beta_1 x\)`
    + Sample Estimation
        - `\(\hat{y} = \widehat{E(y|x)} = \hat{\beta_0} + \hat{\beta_1}x\)`  

* **Law of Probability**  
If A and B are mutually exclusice events then `\(P(A\;or\; B)=P(A)+P(B)\)`  
If A and B are two events, then `\(P(A|B)=\frac{P(A\;and\;B)}{P(B)}\)`    
  
---
 
 * ***Model***

&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-2-1.png" width="45%" height="45%" /&gt;&lt;img src="homoscedasticity.png" width="45%" height="45%" /&gt;
 `\(E(y|x)=\beta_0 + \beta_1 x\)`  
 `\(y=\beta_0 + \beta_1 x +u\)` where `\(u\)` is a random variable with `\(E(u)=0\)` and `\(E(u|x)=0\)`  

--

```
E(wage|educ) = a + b educ

wage = a + b educ + u

```

---


###Week 2

* Random variable

A __random variable__ is a rule that assigns a numerical outcome to an event in each possible state of the world.  

--

 - A __discrete random variable__ has a finite number of distinct outcomes. For example, rolling a die is a random variable with 6 distinct outcomes.  
 
--
 
 - A __continuous random variable__ can take a continuum of values within some interval. For example, rainfall in Melbourne in May can be any number in the range from 0.00 to 200.00 mm.

--

```
Find some examples for both.
```

---



* discrete random variable

The probability density function (pdf) for a discrete random variable X is a function f with f(xi) = pi, i = 1, 2, . . . , m and f(x) = 0 for all other x.

`$$P(X=x)=\begin{cases}
P(X=1) &amp; x=1\\
P(X=0) &amp; x=0\\
\end{cases}$$`

--


`$$P(X=1) = p_1,\ P(X=x_2) = p_2, \ \ldots , P(X=x_m) = p_m$$`

--

* Properties 

--

`$$\sum^m_{i=1} p_i = p_1+p_2+\cdots +p_m= 1$$`

--

`$$0\leq p \leq 1$$`

---

* continuous random variable  

The probability density function (pdf) for a continuous random variable X is a function f such that `\(P(a \leq X \leq b)\)` is the area under the pdf between `\(a\)` and `\(b\)`  

The total area under the pdf is equal to 1.

--


&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

* Expected value 

`$$E(X)=p_1x_1 +p_2x_2+\cdots+p_Mx_m=\sum^m_{i=1}p_ix_i$$`

`$$E(X) = \int^\infty_{-\infty} xf(x)dx$$`

--

* median, mode

--

* Variance  

`$$\sigma^2_X=Var(X)=E(X-\mu_x)^2$$`  

Variance is a measure of spread of the distribution of X around its mean.

If X is an action with different possible outcomes, then Var(X) gives an indication of _riskiness_ of that action.  

--

* Standard deviation   

$$\sigma_X=sd(X)=\sqrt{E(X-\mu_x)^2} $$

In finance, standard deviation is called the _volatility_ in X.  
The advantage of standard deviation over variance is that it has the same units as X.

---

* Properties of the Expected Value  

--

1. For any constant `\(c\)`, `\(E(c)=c\)`.  

--
2. For any constants `\(a\)` and `\(b\)`,  

`$$E(aX+b) = aE(X) +b$$`  

--
3. Expected value is a linear operator, meaning that expected value of sum of several variables is the sum of their expected values:  

`$$E(X+Y+Z)=E(X)+E(Y)+E(Z)$$`

--

`$$E(a +bX+cY+dZ)=a+bE(X)+cE(Y)+dE(Z)$$`  

--
`$$E(X^2)\neq (E(X))^2$$`  

--
`$$E(\log X)\neq \log{(E(X))}$$`  

--
`$$Var(X) = E(X^2) - \mu^2$$`

---





`$$\begin{aligned}
\mathrm{Population\ \ parameter} &amp;\ \ \ \ |\ \ \ \ \mathrm{its\ \ estimater}\\
\mu_y = E(y) &amp;\ \ \ \ |\ \ \ \bar{y}=\frac{1}{n}\sum_{i=1}^{n}{y_i}\\
\sigma_y^2=E(y-\mu_y)^2 &amp;\ \ \ \ |\ \ \ \ s_y^2=\hat{\sigma}_y^2=\frac{1}{n-1}\sum_{i=1}^n{(y_i-\hat{y})^2}\\
\sigma_y=\sqrt{\sigma_y^2} &amp;\ \ \ \ |\ \ \ \ \hat{\sigma_y}=\sqrt{\hat{\sigma}_y^2}\\
\sigma_{xy}=E(x-\mu_x)(y-\mu_y) &amp;\ \ \ \ |\ \ \ \ \hat{\sigma}_{xy}= \frac{1}{n-1}\sum^n_{i=1}{(x_i- \bar{x})(y_i-\bar{y})}\\
\rho_{xy}=\frac{\sigma_{xy}}{\sigma_x\sigma_y}&amp;\ \ \ \ |\ \ \ \ \hat{\rho}_{xy}=\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x \hat{\sigma}_y}\\
E(y|x)=\beta_0 + \beta_1 x &amp;\ \ \ \ |\ \ \ \ \hat{\beta}_1 = \frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x^2}\qquad \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
\end{aligned}$$`


--

```
What do they measure in different contexts?  

```

---

* Diversification in econometrics - Averaging
 
 `$$E(\frac{1}{2}(X_1+X_2))=\frac{1}{2}(\mu +\mu) = \mu$$`  
 
--

`$$\begin{aligned}
Var(\frac{1}{2}(X_1+X_2))&amp;=\frac{1}{4}Var(X_1) + \frac{1}{4}Var(X_2)\\ 
&amp;=\frac{1}{4}(\sigma^2 +\sigma^2) = \sigma^2 /2
\end{aligned}$$`

---

* Sampling and Distribution  



Refer to [here](https://yangzhuoranyang.com/materials/mean_distribution_converge.pdf)

---



* **Law of Probability**  

1. Probability of any event is a number between 0 and 1. The probabilities of all possible outcomes of a random variable add up to 1  

2. If A and B are mutually exclusice events then `\(P(A\;or\; B)=P(A)+P(B)\)`  

3. If A and B are two events, then `\(P(A|B)=\frac{P(A\;and\;B)}{P(B)}\)`  

--

`$$\begin{array}{c|c|c|c|c}
y x &amp;1&amp;2&amp;3&amp;marginal\ f_y \\ 
\hline 
1 &amp; 0.40 &amp;0.24&amp;0.04&amp;\\  
\hline
2 &amp; 0 &amp; 0.16&amp; 0.16&amp;\\ 
\hline
marginal\ f_x &amp;&amp;&amp;&amp;\\ 
\end{array}$$`

--

```
P(y = 1|x = 1) = ?
p(y = 2|x = 2) = ?
```

---

`$$\begin{array}{c|c|c|c|c}
y x &amp;1&amp;2&amp;3&amp;marginal\ f_y \\ 
\hline 
1 &amp; 0.40 &amp;0.24&amp;0.04&amp;\\  
\hline
2 &amp; 0 &amp; 0.16&amp; 0.16&amp;\\ 
\hline
marginal\ f_x &amp;&amp;&amp;&amp;\\ 
\end{array}$$`

`$$P(y=1|x=1)=\frac{P(AB)}{P(B)} =\frac{P(y=1\ \&amp;\ x=1)}{p(x=1)}= \frac{0.4}{0.4} = 1$$`

`$$P(y=2|x=1)=\frac{P(AB)}{P(B)} =\frac{P(y=2\ \&amp;\ x=1)}{p(x=1)}= \frac{0}{0.4} = 0$$`

--

`$$P(y=1|x=2)=$$`

`$$P(y=2|x=2)=$$`
`$$P(y=1|x=3)=$$`

`$$P(y=2|x=3)=$$`

--

```
Expected value?
```

---

`$$\begin{array}{c|c|c|c|c}
y x &amp;1&amp;2&amp;3&amp;marginal\ f_y \\ 
\hline 
1 &amp; 0.40 &amp;0.24&amp;0.04&amp;\\  
\hline
2 &amp; 0 &amp; 0.16&amp; 0.16&amp;\\ 
\hline
marginal\ f_x &amp;&amp;&amp;&amp;\\ 
\end{array}$$`

`$$\begin{aligned}P(y=1|x=1)= 1\ \ \ \ &amp; \ \ \ \ P(y=2|x=1)= 0\\ 
P(y=1|x=2)= 0.6\ \ \ \ &amp; \ \ \ \ P(y=2|x=2)= 0.4\\  
P(y=1|x=3)= 0.2\ \ \ \ &amp; \ \ \ \ P(y=2|x=3)= 0.8
\end{aligned}$$`

--

`$$E(y|x=1)=1 \times P(y=1|x=1) + 2 \times P(y=2|x=1) = 1$$`
`$$E(y|x=2)=$$`
`$$E(y|x=3)=$$`

--

`$$E(y|x=2)=1.4$$`
`$$E(y|x=3)=1.8$$`





---

###Week 3 

A complete model for a random variable is a model of its probability distribution.

e.g. `\(E(y|x)=\beta_0 +\beta_1 x\)`

--

`$$E(y|x)=\alpha +\beta x$$`

When y and x have many possible outcomes or when they are continuous random variables we cannot enumerate the joint density and perform the same exercise.  
--

`$$y = \beta_0 +\beta_1 x + u\ \ \  \text{where}\ \ \ E(u|x)=0$$`
--
Implies that
`$$E(u) = 0$$`
--

* **L.I.E.** (Law of Iterated Expections)

`$$E(E(X|Y))=E(X)$$`

---

Recall: A complete model for a random variable is a model of its probability distribution.

`$$y = \beta_0 +\beta_1 x + u\ \ \  \text{where}\ \ \ E(u|x)=0$$`

--
Add assumptions: 
`$$Var(u|x)=\sigma^2$$`
`$$u|x \sim N$$`  

--
We make the assumption that in the big scheme of things, data are generated by this model, and we want to use observed data to learn the unknowns `\(\beta_0\)`, `\(\beta_1\)` and `\(\sigma^2\)` in order to predict y using x.

---

The method to get there parameters:  

* __The OLS Estimator__

Derivation: Refer to [here](https://yangzhuoranyang.com/materials/OLS_derivation.pdf)

--

`$$\begin{aligned}
\mathrm{Population\ \ parameter} &amp;\ \ \ \ |\ \ \ \ \mathrm{its\ \ estimater}\\
\mu_y = E(y) &amp;\ \ \ \ |\ \ \ \bar{y}=\frac{1}{n}\sum_{i=1}^{n}{y_i}\\
\sigma_y^2=E(y-\mu_y)^2 &amp;\ \ \ \ |\ \ \ \ s_y^2=\hat{\sigma}_y^2=\frac{1}{n-1}\sum_{i=1}^n{(y_i-\hat{y})^2}\\
\sigma_y=\sqrt{\sigma_y^2} &amp;\ \ \ \ |\ \ \ \ \hat{\sigma_y}=\sqrt{\hat{\sigma}_y^2}\\
\sigma_{xy}=E(x-\mu_x)(y-\mu_y) &amp;\ \ \ \ |\ \ \ \ \hat{\sigma}_{xy}= \frac{1}{n-1}\sum^n_{i=1}{(x_i- \bar{x})(y_i-\bar{y})}\\
\rho_{xy}=\frac{\sigma_{xy}}{\sigma_x\sigma_y}&amp;\ \ \ \ |\ \ \ \ \hat{\rho}_{xy}=\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x \hat{\sigma}_y}\\
E(y|x)=\beta_0 + \beta_1 x &amp;\ \ \ \ |\ \ \ \ \hat{\beta}_1 = \frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x^2}\qquad \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
\end{aligned}$$`

---

* Unbiasedness of `\(\hat{\sigma}^2\)` 

`$$\begin{aligned}
s_y^2=\hat{\sigma}_y^2&amp;=\frac{1}{n-1}\sum_{i=1}^n{(y_i-\hat{y})^2}\\
\end{aligned}$$`

--

`$$\begin{aligned}E[\hat{\sigma}_y^2]&amp;=E \left[{\frac {1}{n-1}}\sum_{i=1}^{n}{\big (}y_{i}-{\bar{y}}{\big )}^{2}\right]\\
&amp;=E {\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}{\bigg (}(y_{i}-\mu )-({\bar{y}}-\mu ){\bigg )}^{2}{\bigg ]}\\ 
&amp;=E {\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}{\bigg (}(y_{i}-\mu )^{2}-2({\bar{y}}-\mu )(y_{i}-\mu )+({\bar{y}}-\mu )^{2}{\bigg )}{\bigg ]}\\ 
&amp;=E {\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}(y_{i}-\mu )^{2}-{\frac {2}{n-1}}({\bar{y}}-\mu )\sum _{i=1}^{n}(y_{i}-\mu )+{\frac {n}{n-1}}({\bar{y}}-\mu )^{2}{\bigg ]}\\ 
&amp;=E{\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}(y_{i}-\mu )^{2}-{\frac {2}{n-1}}({\bar{y}}-\mu )\cdot n\cdot(\bar{y}-\mu )+{\frac {n}{n-1}}({\bar{y}}-\mu )^{2}{\bigg ]}\\  
&amp;=E{\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}(y_{i}-\mu )^{2}-{\frac {n}{n-1}}({\bar{y}}-\mu )^{2}{\bigg ]}\\  
&amp;={\frac {1}{n-1}}\sum _{i=1}^{n}E((y_{i}-\mu )^{2})-{\frac {n}{n-1}}E(({\bar{y}}-\mu )^{2})\\  
&amp;=\frac{n}{n-1}\sigma^2 - \frac{n}{n-1}\frac{1}{n}\sigma^2\\ 
&amp;= \sigma^2
\end{aligned}$$`

---

`$$\begin{aligned}E[\hat{\sigma}_y^2]&amp;=E \left[{\frac {1}{n}}\sum_{i=1}^{n}{\big (}y_{i}-{\bar{y}}{\big )}^{2}\right]\\
&amp;=\frac{n}{n}\sigma^2 - \frac{n}{n}\frac{1}{n}\sigma^2\\ 
&amp;= \frac{n-1}{n}\sigma^2
\end{aligned}$$`

---


* Geometry

`$$y_i =\beta_0 +\beta_1 x_i +u_i$$`

`$$\left(\begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n\end{array}\right)=
\left(\begin{array}{c} 1\\ 1\\ \vdots \\ 1\end{array}\right)\beta_0 +
\left(\begin{array}{c}x_1\\ x_2\\ \vdots\\ x_n \end{array}\right)\beta_1 +
\left(\begin{array}{c}u_1\\ u_2\\ \vdots\\ u_n\end{array}\right)$$`


`$$\mathbf{y}=\left(\begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n\end{array}\right)\ \ \mathrm{and}\ \ 
\mathbf{u}=\left(\begin{array}{c} u_1 \\ u_2 \\ \vdots \\ u_n\end{array}\right)\ \ \
\mathbf{X}=\left(\begin{array}{cc} 1 &amp; x_1 \\ 1&amp; x_2 \\ \vdots &amp;\vdots \\ 1&amp;x_n\end{array}\right)\ \ \ 
\mathbf{\beta}=\left(\begin{array}{c} \beta_0 \\ \beta_1 \end{array}\right)$$`


`$$\begin{array}{c} \mathbf{y}\\ n\times 1\end{array}=
\begin{array}{c} \mathbf{X}\\ n\times (k+1)\end{array}
\begin{array}{c} \mathbf{\beta}\\ (k+1)\times 1\end{array}+
\begin{array}{c} \mathbf{u}\\ n\times 1\end{array}$$`


`$$\begin{array}{c} \mathbf{X}\\ n\times (k+1)\end{array}=
\left(\begin{array}{ccccc} 1&amp;x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1k}\\ 1&amp;x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2k}\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\ 1&amp;x_{n1}&amp;x_{n2}&amp;\cdots&amp;x_{nk}\\\end{array}\right)
\ \ \ \ 
\begin{array}{c}\mathbf{\beta}\\ (k+1)\times 1\end{array}= 
\left(\begin{array}{c}\beta_0\\ \beta_1\\  \vdots\\ \beta_k\end{array}\right)$$`

---

* Vector
    + `\(length(\mathbf{u})=(\mathbf{u}'\mathbf{u})^{1/2}\)`  
    
    $$length\left[\begin{array}{c}4\\ 4\end{array}\right]=
\left(\left[\begin{array}{cc}4&amp;4\end{array}\right]\left[\begin{array}{c}4\\ 4\end{array}\right]\right)^{1/2}=\sqrt{32}$$   
    + For `\(\mathbf{u}\)` and `\(\mathbf{v}\)` of the same dimension
        - `\(\mathbf{u}'\mathbf{v}=0 \Leftrightarrow \mathbf{u}\ \mathrm{and}\ \mathbf{v}\ \mathrm{are\ perpendicular\ (orthogonal)\ to\ wach\ other}\)`
        
`$$\left[\begin{array}{cc}4&amp;0\end{array}\right]\left[\begin{array}{c}0\\ 4\end{array}\right]=0$$`  

&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-4-1.png" width="45%" height="45%" /&gt;&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-4-2.png" width="45%" height="45%" /&gt;


`\(\mathbf{Y}\)` be explained as a combination of columns of `\(\mathbf{X}\)` (the column space of `\(\mathbf{X}\)`) with zero `\(\mathbf{\hat{u}}\)` (orthogonal to each other)


---

* __2D and 3D graphic demonstration for OLS__

Refer to [here](https://yangzhuoranyang.com/materials/ols_slides#1)

---

* ***Derivation of `\(\mathbf{\hat{\beta}}\)` in matrix***

`$$\begin{aligned}
\mathbf{y}=\mathbf{X}\mathbf{\hat{\beta}}+\mathbf{\hat{u}} &amp;\Rightarrow \mathbf{\hat{u}}=\mathbf{y}-\mathbf{X}\mathbf{\hat{\beta}}\\   
\mathbf{X}'\mathbf{\hat{u}}&amp;=0\\  
\mathbf{X}'(\mathbf{y}-\mathbf{X}\mathbf{\hat{\beta}})=0 &amp;\Rightarrow \mathbf{X}'\mathbf{y} = \mathbf{X}'\mathbf{X}\mathbf{\hat{\beta}}\\   
&amp;\Rightarrow \mathbf{\hat{\beta}}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}
\end{aligned}$$`


For `\(\mathbf{X}'\mathbf{X}\)` to be invertible ( `\((\mathbf{X}'\mathbf{X})^{-1}\)` exists while `\((\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})=\mathbf{I}\)` ), columns of `\(\mathbf{X}\)` must be linearly independent.  
The vector of OLS predicted value `\(\mathbf{\hat{y}}\)` is the prthogmal projection of `\(\mathbf{y}\)` in the column space of `\(\mathbf{X}\)`. 
`$$\mathbf{\hat{y}}=\mathbf{X}\mathbf{\hat{\beta}}=\mathbf{X(X'X)^{-1}X'y}$$`



&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

Dan's 3D linear regression plane refers to [here](https://yangzhuoranyang.com/materials/Fitting%20a%20plane_Dan.png)

---

`\(\mathbf{y}=\mathbf{\hat{y}}+\mathbf{\hat{u}}\)` 

`\(\mathbf{y}\)`, `\(\mathbf{\hat{y}}\)` and `\(\mathbf{\hat{u}}\)` form a right-angled triangle.
+ `\(length(\mathbf{\hat{y}})=(\mathbf{\hat{y}}'\mathbf{\hat{y}})^{1/2}\)`   `\(\Rightarrow\)`  
`$$\mathbf{y}'\mathbf{y}=\mathbf{\hat{y}}'\mathbf{\hat{y}}+\mathbf{\hat{u}}'\mathbf{\hat{u}}$$`
i.e. `$$\sum_{i=1}^n{y_i^2}=\sum_{i=1}^n{\hat{y}_i^2}+\sum_{i=1}^n{\hat{u}_i^2}$$`
Subtracting `\(n\bar{y}^2\)` from both sides
`$$\sum_{i=1}^n{(y_i-\bar{y})^2}=\sum_{i=1}^n{(\hat{y}_i-\bar{y})^2}+\sum_{i=1}^n{\hat{u}_i^2}$$`
Using `\(\sum^n_{i=1}y_i=\sum^n_{i=1}\hat{y}_i\Rightarrow\bar{y}=\bar{\hat{y}}\)` since `\((1\ \ 1\ \ \cdots\ \ 1)\hat{\mathbf{u}} =0\)`
i.e.  
`$$SST=SSE+SSR$$`

`$$R^2=\frac{SSE}{SST}=1-\frac{SSR}{SST}$$`  




---

###Week 4




`$$\mathrm{Independent\;\; properties}$$`
`$$P(A|B)=P(A)$$`
`$$P(AB)=P(A)\times P(B)$$`
`$$E(XY)=E(x)E(y)$$`
`$$Cov(x,y)=E(xy)-E(x)E(y)=0$$`
`$$Corr(x,y)=0$$`

--
```
Independence and correlation?
```

---

* An estimator (sample `\(\rightarrow\)` population) is *an unbiased estimater* of a parameter of interest if its expected value is the parameter of interest.
* Under the following assumptions `\(E(\hat{\beta})=\beta\)`. i.e. unbiased.  


`$$\begin{array}{c}\mathbf{Multiple\ Regression\ Model\ \ Assumptions} \end{array}$$`

`$$\begin{array}[t]{ &gt;{$}l&lt;{$}|&gt;{$}l&lt;{$} }\hline 
\mathbf{MLR.1\ Linear\ in\ Parameters}\ &amp; \mathbf{E.1\ Linear\ in\ parameters} \\ y = \beta_0 =\beta_1 x_1 + \cdots +\beta_k x_k +u &amp; \mathbf{\underset{n\times 1}{y}=\underset{n\times (k+1)}{X}\underset{(k+1)\times 1}{\beta}+\underset{n\times 1}{u}}\\ \hline
{\mathbf{ MLR.2\ Random\ Sampling}   \\ We\ have\ a\ sample\ of\ n\ observations}\\ \hline   
{\mathbf{ MLR.4\ Zero\ Conditional\ Mean}\\ E(u|x_1, x_2, \cdots, x_k)=0} &amp; 
{\mathbf{E.3\ Zero\ Conditional\ Mean}\\  E(\mathbf{u|X})=\mathbf{\underset{(n\times 1)}{0} }}  \\  
\hline
\mathbf{MLR.3\ No\ Perfect\ Collinearity} &amp; \mathbf{E.2\ No\ Perfect\ Collinearity} \\
None\ of\ x_1,\ x_1,\ \cdots ,\ x_k\ is\ a\ constant\ and\ there\\ are\ no\ exact\ linear\ relationships\ among\ them &amp; \mathbf{X}\ has\ rank\ k+1 \\  
\hline 
\mathbf{MLR.5\ Homoskedasticity}\ &amp; \mathbf{E.4\ Homo\ +\ Randomness} \\ 
Var(u|x_1,\ x_2,\ \cdots ,\ x_k)=\sigma^2 &amp; Var(\mathbf{u}|\mathbf{X})=\sigma^2 \mathbf{I}_n \\ 
\hline
\mathbf{MLR.6\ Normality} &amp; \mathbf{E.5\ Normality}\\ 
Conditional\ on\ \mathbf{X}\ the\ population\ errors&amp;\\ are\ normally\ distributed &amp;\\ 
\hline
\end{array}$$`

---

* Results from the assumptions  
E.1-E.3 Ubiasedness  
E.1-E.4 BLUE
E.1-E.5 T-test, F-test

* ***Unbiasedness***  

To show `\(E(\hat{\beta})=\beta\)`, we need **E.2** (E.2 `\(\Rightarrow\)` `\(\mathbf{X'X}\)` can have a inverse).  
Using **E.1**  `$$\mathbf{\hat{\beta}}=(\mathbf{X'X})^{-1}\mathbf{X'y}=(\mathbf{X'X})^{-1}\mathbf{X'(X\beta+u)}=\mathbf{\beta} + (\mathbf{X'X})^{-1}\mathbf{X'u}$$`  
`$$E(\mathbf{\hat{\beta}})=E[\mathbf{\beta} + (\mathbf{X'X})^{-1}\mathbf{X'u}]=\mathbf{\beta}+E( (\mathbf{X'X})^{-1}\mathbf{X'u}) {\overset{\mathbf{E.3}}{=}} \mathbf{\beta}$$`  
since `\(E(\mathbf{u}|\mathbf{X})=0\Rightarrow E( (\mathbf{X'X})^{-1}\mathbf{X'u})=0\)`

Zero conditional mean is not a problem for _predictive analysis_ because the x set is all we have, we don't want to and won't get causal effect.  


---

However, for _presctiptive analysis_, if we don't controll variables that may be affect by the x we are interested in, we won't get causal effect.  

True model:

`$$wage=\beta_0+\beta_1educ+\beta_2ability +u$$`

Estimate: 

`$$wage=\hat{\alpha}_0+\hat{\alpha}_1educ+\hat{v}$$`

`$$E(\hat{\alpha}_1)=\beta_1+\beta_2\frac{\partial\ ability}{\partial\ educ}\neq\beta_1$$`
"omitted variable bais"  


--

Zero conditional mean  requires  **Strictly exogenous** in time series `\(E(u_t|X_{s1}, X_{s2}, \cdots ,X_{sk})=0\forall s=1,2,\cdots , T\)`  
implication: the error term in any time period t is uncorrelated with each of the regressors in all time periods, past, present, and future.  
`$$Corr(u_t,x_{11})=\cdots=Corr(u_t,x_{T1})=Corr(u_t,x_{1k})=\cdots=Corr(u_t,x_{Tk})=0$$`

It is violated when the regression model contains a lag of the dependent variable as a regressor.

---

* variance-covariance matrix  

For a random variable `\(\mathbf{v}\)` and its expectation 

`$$\mathbf{v}=\left(\begin{array}{c} v_1\\ v_2\\ \vdots\\ v_n \end{array}\right)\qquad E(\mathbf{v})=\underset{n\times 1}{\mu}=\left(\begin{array}{c} \mu_1\\ \mu_2\\ \vdots\\ \mu_n\end{array} \right)$$`  
 
variance `\(\sigma^2_i\)` covariance `\(\sigma^2_{ij}\)`  
`\(Var({\mathbf{A}'\mathbf{v}})=\mathbf{A}'Var({\mathbf{v}})\mathbf{A}\)`  
 
The variance-covariance matrix for `\(\mathbf{v}\)` 
`$$Var(\mathbf{v})=E(\mathbf{v}-\mathbf{\mu})(\mathbf{v}-\mathbf{\mu})'=\left[\begin{array}{cccc} \sigma^2_1 &amp;\sigma_{12} &amp; \cdots &amp; \sigma_{1n} \\ \sigma_{21} &amp; \sigma^2_2 &amp; \cdots &amp; \sigma_{2n}\\ \vdots &amp;\vdots&amp;\ &amp;\vdots\\ \sigma_{n1} &amp; \sigma_{n2} &amp; \cdots &amp; \sigma_n^2\end{array}\right]$$`

---

* The variance of OLS estimator  

`$$Var(\mathbf{\hat{\beta}}|\mathbf{X})=\sigma^2(\mathbf{X'X})^{-1}$$`
Proof: `$$\mathbf{\hat{\beta}}=(\mathbf{X'X})^{-1}\mathbf{X'y}=(\mathbf{X'X})^{-1}\mathbf{X'(X\beta+u)}=\mathbf{\beta} + \underset{\mathbf{A}'}{\underbrace{(\mathbf{X'X})^{-1}\mathbf{X}'}}\mathbf{u}$$`  
`$$\begin{aligned}Var(\mathbf{\hat{\beta}}|\mathbf{X}) &amp;=Var(\mathbf{\beta} +(\mathbf{X'X})^{-1}\mathbf{X'u}|\mathbf{A})\\ &amp;=Var(\mathbf{A}'\mathbf{u}|\mathbf{A})\\ &amp;=\mathbf{A}'Var(\mathbf{u})\mathbf{A}\\ E.5\ \Rightarrow Var(\mathbf{\hat{\beta}})&amp;=\mathbf{A}'\sigma^2\mathbf{I}_n\mathbf{A}\\ &amp;=\sigma^2(\mathbf{X'X})^{-1}\mathbf{X}'\mathbf{X}(\mathbf{X'X})^{-1}\\ &amp;=\sigma^2(\mathbf{X'X})^{-1}\end{aligned}$$`  

---

To estimate `\(\sigma^2\)`, we can use the unbiased estiomator 
`$$\hat{\sigma}^2=\frac{\sum^n_{i=1}\hat{u}^2_i}{n-k-1}=\frac{\hat{\mathbf{u}}'\hat{\mathbf{u}}}{n-k-1}$$`

Think about dimensions: `\(\hat{y}\)` is in the column space of `\(X\)`, so it is in a subspace with dimension `\(k + 1\)`   

`\(\hat{\mathbf{u}}\)` is orthogonal to column space of `\(\mathbf{X}\)`, so it is in a subspace with dimension `\(n-(k+1)=n-k-1\)`. So even though there are n coordinates in `\(\hat{\mathbf{u}}\)`, only `\(n − k − 1\)` of those are free (it has n − k − 1 _degree of freedom_)

--

* *B.L.U.E.*  
   + What is BLUE
      -   Best(smallest variance) Linear Unbiased Estimator  
   + How to prove BLUE  
      -   Using E.1-E.4, based on Gauss-Markov Theorem, `\(\mathbf{\hat{\beta}}\)` is BLUE of `\(\mathbf{\beta}\)`  
      
      
---



* ***Interpretation***

`$$Murder=\beta_0+\beta_1 Assault + \beta_2 Population + u$$`

`$$\widehat{Murder}=\underset{(1.74)}{3.21}+\underset{0.005}{0.044}Assult-\underset{0.027}{0.045}UrbanPop$$`



```
## 
## Call:
## lm(formula = Murder ~ Assault + UrbanPop, data = USArrests)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5530 -1.7093 -0.3677  1.2284  7.5985 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.207153   1.740790   1.842   0.0717 .  
## Assault      0.043910   0.004579   9.590 1.22e-12 ***
## UrbanPop    -0.044510   0.026363  -1.688   0.0980 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.58 on 47 degrees of freedom
## Multiple R-squared:  0.6634,	Adjusted R-squared:  0.6491 
## F-statistic: 46.32 on 2 and 47 DF,  p-value: 7.704e-12
```

---

* Rescaling (e.g. change the unit)  
 + do not create any new information, so it only changes OLS results in predictable and non-substantive ways.  
 
 `$$\begin{aligned} \hat{y}=\hat{\beta_0}+\hat{\beta_1}x_1+\hat{\beta_2}x_2&amp;\Rightarrow \hat{y}=\hat{\beta_0^*}+\hat{\beta_1^*}x_1^*+\hat{\beta_2^*}x_2\\ 1.\quad x_1\rightarrow cx_1&amp;\quad x_1^*=cx_1\\  \Rightarrow \hat{\beta_0^*}&amp;= \hat{\beta_0} \quad \hat{\beta_1^*}= \hat{\beta_1}/c\quad \hat{\beta_2^*}= \hat{\beta_2}\\  2.\quad x_1\rightarrow a+cx_1&amp;\quad x_1^*=a+cx_1\\  \Rightarrow \hat{\beta_0^*}&amp;= \hat{\beta_0}-\hat{\beta_1}/c\quad \hat{\beta_1^*}= \hat{\beta_1}/c\quad \hat{\beta_2^*}= \hat{\beta_2}\\  \end{aligned}$$`

Since `\(\hat{y}\)` does not change residuals, SST, SSE, SSR stay the same, so `\(R^2\)` will not change.

--

`$$\begin{aligned} y=\hat{\beta_0}+\hat{\beta_1}x_1+\hat{\beta_2}x_2+\hat{u}&amp;\Rightarrow y=\hat{\beta_0^*}+\hat{\beta_1^*}x_1^*+\hat{\beta_2^*}x_2+\hat{u}^*\\ y\rightarrow cy\quad y^*&amp;=cy\\ \Rightarrow \hat{\beta_0^*}&amp;= c\hat{\beta_0}\quad \hat{\beta_1^*}= c\hat{\beta_1}\quad \hat{\beta_2^*}= c\hat{\beta_2}\quad \hat{u}^*=c\hat{u}\\ \end{aligned}$$`  

SST, SSE, SSR all change (multiplied by) same amount, so `\(R^2\)` will not change. 


---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"includes": {
"in_header": "mylatexpackage.sty"
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
