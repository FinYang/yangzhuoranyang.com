<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>ETC2410</title>
    <meta charset="utf-8" />
    <meta name="author" content="Fin" />
    <link href="ETC2410_slides_S1_2019_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="ETC2410_slides_S1_2019_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ETC2410
### Fin
### Semester 1 2019

---




###Week 1

* Stages of empirical analysis

1. Understanding the problem
2. Formulating an appropriate conceptual model to tackle the problem
3. Collecting appropriate data
4. Looking at the data (Descriptive Analytics)
5. Estimating the model, making inference, predictions and policy presciptions as appropriate
6. Evaluation, learning and improving each of the previous steps, and iterating until the problem is solved 

--

```
e.g.
What elements will affect earnings?

```

---

* Converting an economic model to a statistical model  

 - conomic theory describes the systematic part - `\(f (i)\)`
 - `\(e\)` is the nonsystematic, random error component that we know is present, but cannot be observed
 - Adding random errors converts our economic model into a statistical one that gives us a basis for statistical inference
 
--
* Experimental and Non-experimental data  

--

 __Non-experimental__ data are not accumulated through controlled experiments on individuals, firms, or segments of the economy.  
 (observational data, retrospective data)  
 
--
 
 __Experimental__ data are often collected in laboratory environments in the natural sciences, but they are much more difficult to obtain in the social sciences.  

--

```
What data we want to collect about wage?
```

---

---

* Predictive analytics (no causality) and Prescriptive analytics (causal)

--

__Predictive__ analytics: using some variables to predict a target variable without any requirement of causality

--

__Prescriptive__ analytics to measure the causal relationship between variables, to prescribe how to achieve a desired change in the target variable by manipulating the cause.

--

We can always use multiple regression for prediction.  
We can sometimes use multiple regression to tease out causal relationships

--

```
Which one is for analyzing wage?
```


---



* Cross-sectional, Time-series, Panel or Longitudinal data, Pooled
 
--

A __Cross-sectional__ data set consists of a sample of individuals, households, firms, cities, states, contries, or a variety of other units, _taken at a given point in time_.  

--

A __Time Series__ data set consists of observations on a variable or several variables _over time_.  

--

A __Pooled__ data set combine data (on multiple variable) from _different individuals_ over time.  

--

A __Panel or Longitudinal__ data set consists of a time series for _each_ cross-sectional member in the data set.

--

```
Which one is for wage related data?
```

---

* **What time series data can do, but cross-section cannot:**  

 - Forecast future values of a variable  
 - Estimate the dynamic causal effect of one variable `\(x\)` (estimate the causal effect on y, over several time periods, of a change in `\(x\)` today. e.g. tax on alcohol)  

--

1.  Forecast future values of a variable:  
eg. stock prices, consumer price index, gross domestic product, annual homicide rates one or several days /months /quarters / years ahead.  

--

2.  Estimate the dynamic causal effect of one variable x on another variable y:   
eg. estimate the effect on alcohol consumption of an increase in the tax on alcohol, both initially and subsequently as consumers adjust to the new tax.  

--

```
wage?
```

---


* **Cross-sectional data, time series data, panel data**  
 Panel data: can be used to address questions that cannot be adequately addressed using either cross-section or time series data.
 
--
 
 - **Univariate time series**: A time series data set consisting of observations on a single variable.  
 - **Multivariate time series**: A time series data set consisting of observations on several variables.  
 
--
 
* Notation  
`\(y_t\)` : the value of the time series in time period t.  
`\(T\)` : sample size  

---



* **Properties**



  1. Observations on time series data are ordered.
  
--
   
  2. Time series data is generally characterized by some form of temporal dependence.  
  
--
  
  Because of temporal dependence, it is implaisible to assume that the random variable `\(y_t\)` and `\(y_{t-1}\)` are i.i.d.  
      The strength of temporal dependence can differ between time series. When the dependence is strong the time series is called _persistent_.

---

* Notation  

cross section analysis:  

`$$y_i = \beta_0 +\beta_1 x_{i1} +\beta_2x_{i2} + \ldots +\beta_kx_{ik} +u_i\ , \ \  \ i = 1, 2,\ldots, n$$`

--

time series analysis:  

`$$y_t = \beta_0 +\beta_1 x_{t1} +\beta_2x_{t2} + \ldots +\beta_kx_{tk} +u_t\ , \ \  \ t = 1, 2,\ldots, n$$`

---

* A general statistical model can be written as: 

`$$y=f(x)+e$$`

Where y denotes the random variablewe want to predict or analyze; x is the variable that are related or can explain some behaviour of y; random error e accounts for the many factors that affect y that we have omitted from this simple model



&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-1-1.png" width="45%" height="45%" /&gt;&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-1-2.png" width="45%" height="45%" /&gt;

--

```
Write a general model form for wage.
```



---

* Simple Linear Regression  
A complete model for a random variable is a model of its probability distribution. We model the conditional distribution of y given x, a model for `\(E(y|x)\)`.  

`$$\begin{array}{c|c}
y &amp;x \\
\hline Dependent\ \ Variable &amp; Independent\ \ Variable\\ 
Response\ \ Variable &amp; Control\ \  Variable\\ 
Explained\ \ Variable &amp; Explanatory\ \  Variable\\ 
Predicted\ \  Variable &amp; Predictor\ \  Variable\\ 
Regressand &amp; Regressor\\ 
\end{array}$$`


* Population and Sample format
    + Population 
        - `\(E(y|x)=\beta_0 + \beta_1 x\)`
    + Sample Estimation
        - `\(\hat{y} = \widehat{E(y|x)} = \hat{\beta_0} + \hat{\beta_1}x\)`  

* **Law of Probability**  
If A and B are mutually exclusice events then `\(P(A\;or\; B)=P(A)+P(B)\)`  
If A and B are two events, then `\(P(A|B)=\frac{P(A\;and\;B)}{P(B)}\)`    
  
---
 
 * ***Model***

&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-2-1.png" width="45%" height="45%" /&gt;&lt;img src="homoscedasticity.png" width="45%" height="45%" /&gt;
 `\(E(y|x)=\beta_0 + \beta_1 x\)`  
 `\(y=\beta_0 + \beta_1 x +u\)` where `\(u\)` is a random variable with `\(E(u)=0\)` and `\(E(u|x)=0\)`  

--

```
E(wage|educ) = a + b educ

wage = a + b educ + u

```

---


###Week 2

* Random variable

A __random variable__ is a rule that assigns a numerical outcome to an event in each possible state of the world.  

--

 - A __discrete random variable__ has a finite number of distinct outcomes. For example, rolling a die is a random variable with 6 distinct outcomes.  
 
--
 
 - A __continuous random variable__ can take a continuum of values within some interval. For example, rainfall in Melbourne in May can be any number in the range from 0.00 to 200.00 mm.

--

```
Find some examples for both.
```

---



* discrete random variable

The probability density function (pdf) for a discrete random variable X is a function f with f(xi) = pi, i = 1, 2, . . . , m and f(x) = 0 for all other x.

`$$P(X=x)=\begin{cases}
P(X=1) &amp; x=1\\
P(X=0) &amp; x=0\\
\end{cases}$$`

--


`$$P(X=1) = p_1,\ P(X=x_2) = p_2, \ \ldots , P(X=x_m) = p_m$$`

--

* Properties 

--

`$$\sum^m_{i=1} p_i = p_1+p_2+\cdots +p_m= 1$$`

--

`$$0\leq p \leq 1$$`

---

* continuous random variable  

The probability density function (pdf) for a continuous random variable X is a function f such that `\(P(a \leq X \leq b)\)` is the area under the pdf between `\(a\)` and `\(b\)`  

The total area under the pdf is equal to 1.

--


&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-3-1.png" style="display: block; margin: auto;" /&gt;

---

* Expected value 

`$$E(X)=p_1x_1 +p_2x_2+\cdots+p_Mx_m=\sum^m_{i=1}p_ix_i$$`

`$$E(X) = \int^\infty_{-\infty} xf(x)dx$$`

--

* median, mode

--

* Variance  

`$$\sigma^2_X=Var(X)=E(X-\mu_x)^2$$`  

Variance is a measure of spread of the distribution of X around its mean.

If X is an action with different possible outcomes, then Var(X) gives an indication of _riskiness_ of that action.  

--

* Standard deviation   

$$\sigma_X=sd(X)=\sqrt{E(X-\mu_x)^2} $$

In finance, standard deviation is called the _volatility_ in X.  
The advantage of standard deviation over variance is that it has the same units as X.

---

* Properties of the Expected Value  

--

1. For any constant `\(c\)`, `\(E(c)=c\)`.  

--
2. For any constants `\(a\)` and `\(b\)`,  

`$$E(aX+b) = aE(X) +b$$`  

--
3. Expected value is a linear operator, meaning that expected value of sum of several variables is the sum of their expected values:  

`$$E(X+Y+Z)=E(X)+E(Y)+E(Z)$$`

--

`$$E(a +bX+cY+dZ)=a+bE(X)+cE(Y)+dE(Z)$$`  

--
`$$E(X^2)\neq (E(X))^2$$`  

--
`$$E(\log X)\neq \log{(E(X))}$$`  

--
`$$Var(X) = E(X^2) - \mu^2$$`

---





`$$\begin{aligned}
\mathrm{Population\ \ parameter} &amp;\ \ \ \ |\ \ \ \ \mathrm{its\ \ estimater}\\
\mu_y = E(y) &amp;\ \ \ \ |\ \ \ \bar{y}=\frac{1}{n}\sum_{i=1}^{n}{y_i}\\
\sigma_y^2=E(y-\mu_y)^2 &amp;\ \ \ \ |\ \ \ \ s_y^2=\hat{\sigma}_y^2=\frac{1}{n-1}\sum_{i=1}^n{(y_i-\hat{y})^2}\\
\sigma_y=\sqrt{\sigma_y^2} &amp;\ \ \ \ |\ \ \ \ \hat{\sigma_y}=\sqrt{\hat{\sigma}_y^2}\\
\sigma_{xy}=E(x-\mu_x)(y-\mu_y) &amp;\ \ \ \ |\ \ \ \ \hat{\sigma}_{xy}= \frac{1}{n-1}\sum^n_{i=1}{(x_i- \bar{x})(y_i-\bar{y})}\\
\rho_{xy}=\frac{\sigma_{xy}}{\sigma_x\sigma_y}&amp;\ \ \ \ |\ \ \ \ \hat{\rho}_{xy}=\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x \hat{\sigma}_y}\\
E(y|x)=\beta_0 + \beta_1 x &amp;\ \ \ \ |\ \ \ \ \hat{\beta}_1 = \frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x^2}\qquad \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
\end{aligned}$$`


--

```
What do they measure in different contexts?  

```

---

* Diversification in econometrics - Averaging
 
 `$$E(\frac{1}{2}(X_1+X_2))=\frac{1}{2}(\mu +\mu) = \mu$$`  
 
--

`$$\begin{aligned}
Var(\frac{1}{2}(X_1+X_2))&amp;=\frac{1}{4}Var(X_1) + \frac{1}{4}Var(X_2)\\ 
&amp;=\frac{1}{4}(\sigma^2 +\sigma^2) = \sigma^2 /2
\end{aligned}$$`

---

* Sampling and Distribution  



Refer to [here](https://yangzhuoranyang.com/materials/mean_distribution_converge.pdf)

---



* **Law of Probability**  

1. Probability of any event is a number between 0 and 1. The probabilities of all possible outcomes of a random variable add up to 1  

2. If A and B are mutually exclusice events then `\(P(A\;or\; B)=P(A)+P(B)\)`  

3. If A and B are two events, then `\(P(A|B)=\frac{P(A\;and\;B)}{P(B)}\)`  

--

`$$\begin{array}{c|c|c|c|c}
y x &amp;1&amp;2&amp;3&amp;marginal\ f_y \\ 
\hline 
1 &amp; 0.40 &amp;0.24&amp;0.04&amp;\\  
\hline
2 &amp; 0 &amp; 0.16&amp; 0.16&amp;\\ 
\hline
marginal\ f_x &amp;&amp;&amp;&amp;\\ 
\end{array}$$`

--

```
P(y = 1|x = 1) = ?
p(y = 2|x = 2) = ?
```

---

`$$\begin{array}{c|c|c|c|c}
y x &amp;1&amp;2&amp;3&amp;marginal\ f_y \\ 
\hline 
1 &amp; 0.40 &amp;0.24&amp;0.04&amp;\\  
\hline
2 &amp; 0 &amp; 0.16&amp; 0.16&amp;\\ 
\hline
marginal\ f_x &amp;&amp;&amp;&amp;\\ 
\end{array}$$`

`$$P(y=1|x=1)=\frac{P(AB)}{P(B)} =\frac{P(y=1\ \&amp;\ x=1)}{p(x=1)}= \frac{0.4}{0.4} = 1$$`

`$$P(y=2|x=1)=\frac{P(AB)}{P(B)} =\frac{P(y=2\ \&amp;\ x=1)}{p(x=1)}= \frac{0}{0.4} = 0$$`

--

`$$P(y=1|x=2)=$$`

`$$P(y=2|x=2)=$$`
`$$P(y=1|x=3)=$$`

`$$P(y=2|x=3)=$$`

--

```
Expected value?
```

---

`$$\begin{array}{c|c|c|c|c}
y x &amp;1&amp;2&amp;3&amp;marginal\ f_y \\ 
\hline 
1 &amp; 0.40 &amp;0.24&amp;0.04&amp;\\  
\hline
2 &amp; 0 &amp; 0.16&amp; 0.16&amp;\\ 
\hline
marginal\ f_x &amp;&amp;&amp;&amp;\\ 
\end{array}$$`

`$$\begin{aligned}P(y=1|x=1)= 1\ \ \ \ &amp; \ \ \ \ P(y=2|x=1)= 0\\ 
P(y=1|x=2)= 0.6\ \ \ \ &amp; \ \ \ \ P(y=2|x=2)= 0.4\\  
P(y=1|x=3)= 0.2\ \ \ \ &amp; \ \ \ \ P(y=2|x=3)= 0.8
\end{aligned}$$`

--

`$$E(y|x=1)=1 \times P(y=1|x=1) + 2 \times P(y=2|x=1) = 1$$`
`$$E(y|x=2)=$$`
`$$E(y|x=3)=$$`

--

`$$E(y|x=2)=1.4$$`
`$$E(y|x=3)=1.8$$`





---

###Week 3 

A complete model for a random variable is a model of its probability distribution.

e.g. `\(E(y|x)=\beta_0 +\beta_1 x\)`

--

`$$E(y|x)=\alpha +\beta x$$`

When y and x have many possible outcomes or when they are continuous random variables we cannot enumerate the joint density and perform the same exercise.  
--

`$$y = \beta_0 +\beta_1 x + u\ \ \  \text{where}\ \ \ E(u|x)=0$$`
--
Implies that
`$$E(u) = 0$$`
--

* **L.I.E.** (Law of Iterated Expections)

`$$E(E(X|Y))=E(X)$$`

---

Recall: A complete model for a random variable is a model of its probability distribution.

`$$y = \beta_0 +\beta_1 x + u\ \ \  \text{where}\ \ \ E(u|x)=0$$`

--
Add assumptions: 
`$$Var(u|x)=\sigma^2$$`
`$$u|x \sim N$$`  

--
We make the assumption that in the big scheme of things, data are generated by this model, and we want to use observed data to learn the unknowns `\(\beta_0\)`, `\(\beta_1\)` and `\(\sigma^2\)` in order to predict y using x.

---

The method to get there parameters:  

* __The OLS Estimator__

Derivation: Refer to [here](https://yangzhuoranyang.com/materials/OLS_derivation.pdf)

--

`$$\begin{aligned}
\mathrm{Population\ \ parameter} &amp;\ \ \ \ |\ \ \ \ \mathrm{its\ \ estimater}\\
\mu_y = E(y) &amp;\ \ \ \ |\ \ \ \bar{y}=\frac{1}{n}\sum_{i=1}^{n}{y_i}\\
\sigma_y^2=E(y-\mu_y)^2 &amp;\ \ \ \ |\ \ \ \ s_y^2=\hat{\sigma}_y^2=\frac{1}{n-1}\sum_{i=1}^n{(y_i-\hat{y})^2}\\
\sigma_y=\sqrt{\sigma_y^2} &amp;\ \ \ \ |\ \ \ \ \hat{\sigma_y}=\sqrt{\hat{\sigma}_y^2}\\
\sigma_{xy}=E(x-\mu_x)(y-\mu_y) &amp;\ \ \ \ |\ \ \ \ \hat{\sigma}_{xy}= \frac{1}{n-1}\sum^n_{i=1}{(x_i- \bar{x})(y_i-\bar{y})}\\
\rho_{xy}=\frac{\sigma_{xy}}{\sigma_x\sigma_y}&amp;\ \ \ \ |\ \ \ \ \hat{\rho}_{xy}=\frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x \hat{\sigma}_y}\\
E(y|x)=\beta_0 + \beta_1 x &amp;\ \ \ \ |\ \ \ \ \hat{\beta}_1 = \frac{\hat{\sigma}_{xy}}{\hat{\sigma}_x^2}\qquad \hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x}
\end{aligned}$$`

---

* Unbiasedness of `\(\hat{\sigma}^2\)` 

`$$\begin{aligned}
s_y^2=\hat{\sigma}_y^2&amp;=\frac{1}{n-1}\sum_{i=1}^n{(y_i-\hat{y})^2}\\
\end{aligned}$$`

--

`$$\begin{aligned}E[\hat{\sigma}_y^2]&amp;=E \left[{\frac {1}{n-1}}\sum_{i=1}^{n}{\big (}y_{i}-{\bar{y}}{\big )}^{2}\right]\\
&amp;=E {\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}{\bigg (}(y_{i}-\mu )-({\bar{y}}-\mu ){\bigg )}^{2}{\bigg ]}\\ 
&amp;=E {\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}{\bigg (}(y_{i}-\mu )^{2}-2({\bar{y}}-\mu )(y_{i}-\mu )+({\bar{y}}-\mu )^{2}{\bigg )}{\bigg ]}\\ 
&amp;=E {\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}(y_{i}-\mu )^{2}-{\frac {2}{n-1}}({\bar{y}}-\mu )\sum _{i=1}^{n}(y_{i}-\mu )+{\frac {n}{n-1}}({\bar{y}}-\mu )^{2}{\bigg ]}\\ 
&amp;=E{\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}(y_{i}-\mu )^{2}-{\frac {2}{n-1}}({\bar{y}}-\mu )\cdot n\cdot(\bar{y}-\mu )+{\frac {n}{n-1}}({\bar{y}}-\mu )^{2}{\bigg ]}\\  
&amp;=E{\bigg [}{\frac {1}{n-1}}\sum _{i=1}^{n}(y_{i}-\mu )^{2}-{\frac {n}{n-1}}({\bar{y}}-\mu )^{2}{\bigg ]}\\  
&amp;={\frac {1}{n-1}}\sum _{i=1}^{n}E((y_{i}-\mu )^{2})-{\frac {n}{n-1}}E(({\bar{y}}-\mu )^{2})\\  
&amp;=\frac{n}{n-1}\sigma^2 - \frac{n}{n-1}\frac{1}{n}\sigma^2\\ 
&amp;= \sigma^2
\end{aligned}$$`

---

`$$\begin{aligned}E[\hat{\sigma}_y^2]&amp;=E \left[{\frac {1}{n}}\sum_{i=1}^{n}{\big (}y_{i}-{\bar{y}}{\big )}^{2}\right]\\
&amp;=\frac{n}{n}\sigma^2 - \frac{n}{n}\frac{1}{n}\sigma^2\\ 
&amp;= \frac{n-1}{n}\sigma^2
\end{aligned}$$`

---


* Geometry

`$$y_i =\beta_0 +\beta_1 x_i +u_i$$`

`$$\left(\begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n\end{array}\right)=
\left(\begin{array}{c} 1\\ 1\\ \vdots \\ 1\end{array}\right)\beta_0 +
\left(\begin{array}{c}x_1\\ x_2\\ \vdots\\ x_n \end{array}\right)\beta_1 +
\left(\begin{array}{c}u_1\\ u_2\\ \vdots\\ u_n\end{array}\right)$$`


`$$\mathbf{y}=\left(\begin{array}{c} y_1 \\ y_2 \\ \vdots \\ y_n\end{array}\right)\ \ \mathrm{and}\ \ 
\mathbf{u}=\left(\begin{array}{c} u_1 \\ u_2 \\ \vdots \\ u_n\end{array}\right)\ \ \
\mathbf{X}=\left(\begin{array}{cc} 1 &amp; x_1 \\ 1&amp; x_2 \\ \vdots &amp;\vdots \\ 1&amp;x_n\end{array}\right)\ \ \ 
\mathbf{\beta}=\left(\begin{array}{c} \beta_0 \\ \beta_1 \end{array}\right)$$`


`$$\begin{array}{c} \mathbf{y}\\ n\times 1\end{array}=
\begin{array}{c} \mathbf{X}\\ n\times (k+1)\end{array}
\begin{array}{c} \mathbf{\beta}\\ (k+1)\times 1\end{array}+
\begin{array}{c} \mathbf{u}\\ n\times 1\end{array}$$`


`$$\begin{array}{c} \mathbf{X}\\ n\times (k+1)\end{array}=
\left(\begin{array}{ccccc} 1&amp;x_{11}&amp;x_{12}&amp;\cdots&amp;x_{1k}\\ 1&amp;x_{21}&amp;x_{22}&amp;\cdots&amp;x_{2k}\\
\vdots&amp;\vdots&amp;\vdots&amp;&amp;\vdots\\ 1&amp;x_{n1}&amp;x_{n2}&amp;\cdots&amp;x_{nk}\\\end{array}\right)
\ \ \ \ 
\begin{array}{c}\mathbf{\beta}\\ (k+1)\times 1\end{array}= 
\left(\begin{array}{c}\beta_0\\ \beta_1\\  \vdots\\ \beta_k\end{array}\right)$$`

---

* Vector
    + `\(length(\mathbf{u})=(\mathbf{u}'\mathbf{u})^{1/2}\)`  
    
    $$length\left[\begin{array}{c}4\\ 4\end{array}\right]=
\left(\left[\begin{array}{cc}4&amp;4\end{array}\right]\left[\begin{array}{c}4\\ 4\end{array}\right]\right)^{1/2}=\sqrt{32}$$   
    + For `\(\mathbf{u}\)` and `\(\mathbf{v}\)` of the same dimension
        - `\(\mathbf{u}'\mathbf{v}=0 \Leftrightarrow \mathbf{u}\ \mathrm{and}\ \mathbf{v}\ \mathrm{are\ perpendicular\ (orthogonal)\ to\ wach\ other}\)`
        
`$$\left[\begin{array}{cc}4&amp;0\end{array}\right]\left[\begin{array}{c}0\\ 4\end{array}\right]=0$$`  

&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-4-1.png" width="45%" height="45%" /&gt;&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-4-2.png" width="45%" height="45%" /&gt;


`\(\mathbf{Y}\)` be explained as a combination of columns of `\(\mathbf{X}\)` (the column space of `\(\mathbf{X}\)`) with zero `\(\mathbf{\hat{u}}\)` (orthogonal to each other)


---

* __2D and 3D graphic demonstration for OLS__

Refer to [here](https://yangzhuoranyang.com/materials/ols_slides#1)

---

* ***Derivation of `\(\mathbf{\hat{\beta}}\)` in matrix***

`$$\begin{aligned}
\mathbf{y}=\mathbf{X}\mathbf{\hat{\beta}}+\mathbf{\hat{u}} &amp;\Rightarrow \mathbf{\hat{u}}=\mathbf{y}-\mathbf{X}\mathbf{\hat{\beta}}\\   
\mathbf{X}'\mathbf{\hat{u}}&amp;=0\\  
\mathbf{X}'(\mathbf{y}-\mathbf{X}\mathbf{\hat{\beta}})=0 &amp;\Rightarrow \mathbf{X}'\mathbf{y} = \mathbf{X}'\mathbf{X}\mathbf{\hat{\beta}}\\   
&amp;\Rightarrow \mathbf{\hat{\beta}}=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}
\end{aligned}$$`


For `\(\mathbf{X}'\mathbf{X}\)` to be invertible ( `\((\mathbf{X}'\mathbf{X})^{-1}\)` exists while `\((\mathbf{X}'\mathbf{X})^{-1}(\mathbf{X}'\mathbf{X})=\mathbf{I}\)` ), columns of `\(\mathbf{X}\)` must be linearly independent.  
The vector of OLS predicted value `\(\mathbf{\hat{y}}\)` is the prthogmal projection of `\(\mathbf{y}\)` in the column space of `\(\mathbf{X}\)`. 
`$$\mathbf{\hat{y}}=\mathbf{X}\mathbf{\hat{\beta}}=\mathbf{X(X'X)^{-1}X'y}$$`



&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-5-1.png" style="display: block; margin: auto;" /&gt;

Dan's 3D linear regression plane refers to [here](https://yangzhuoranyang.com/materials/Fitting%20a%20plane_Dan.png)

---

`\(\mathbf{y}=\mathbf{\hat{y}}+\mathbf{\hat{u}}\)` 

`\(\mathbf{y}\)`, `\(\mathbf{\hat{y}}\)` and `\(\mathbf{\hat{u}}\)` form a right-angled triangle.
+ `\(length(\mathbf{\hat{y}})=(\mathbf{\hat{y}}'\mathbf{\hat{y}})^{1/2}\)`   `\(\Rightarrow\)`  
`$$\mathbf{y}'\mathbf{y}=\mathbf{\hat{y}}'\mathbf{\hat{y}}+\mathbf{\hat{u}}'\mathbf{\hat{u}}$$`
i.e. `$$\sum_{i=1}^n{y_i^2}=\sum_{i=1}^n{\hat{y}_i^2}+\sum_{i=1}^n{\hat{u}_i^2}$$`
Subtracting `\(n\bar{y}^2\)` from both sides
`$$\sum_{i=1}^n{(y_i-\bar{y})^2}=\sum_{i=1}^n{(\hat{y}_i-\bar{y})^2}+\sum_{i=1}^n{\hat{u}_i^2}$$`
Using `\(\sum^n_{i=1}y_i=\sum^n_{i=1}\hat{y}_i\Rightarrow\bar{y}=\bar{\hat{y}}\)` since `\((1\ \ 1\ \ \cdots\ \ 1)\hat{\mathbf{u}} =0\)`
i.e.  
`$$SST=SSE+SSR$$`

`$$R^2=\frac{SSE}{SST}=1-\frac{SSR}{SST}$$`  




---

###Week 4




`$$\mathrm{Independent\;\; properties}$$`
`$$P(A|B)=P(A)$$`
`$$P(AB)=P(A)\times P(B)$$`
`$$E(XY)=E(x)E(y)$$`
`$$Cov(x,y)=E(xy)-E(x)E(y)=0$$`
`$$Corr(x,y)=0$$`

--
```
Independence and correlation?
```

---

* An estimator (sample `\(\rightarrow\)` population) is *an unbiased estimater* of a parameter of interest if its expected value is the parameter of interest.
* Under the following assumptions `\(E(\hat{\beta})=\beta\)`. i.e. unbiased.  


`$$\begin{array}{c}\mathbf{Multiple\ Regression\ Model\ \ Assumptions} \end{array}$$`

`$$\begin{array}[t]{ &gt;{$}l&lt;{$}|&gt;{$}l&lt;{$} }\hline 
\mathbf{MLR.1\ Linear\ in\ Parameters}\ &amp; \mathbf{E.1\ Linear\ in\ parameters} \\ y = \beta_0 =\beta_1 x_1 + \cdots +\beta_k x_k +u &amp; \mathbf{\underset{n\times 1}{y}=\underset{n\times (k+1)}{X}\underset{(k+1)\times 1}{\beta}+\underset{n\times 1}{u}}\\ \hline
{\mathbf{ MLR.2\ Random\ Sampling}   \\ We\ have\ a\ sample\ of\ n\ observations}\\ \hline   
{\mathbf{ MLR.4\ Zero\ Conditional\ Mean}\\ E(u|x_1, x_2, \cdots, x_k)=0} &amp; 
{\mathbf{E.3\ Zero\ Conditional\ Mean}\\  E(\mathbf{u|X})=\mathbf{\underset{(n\times 1)}{0} }}  \\  
\hline
\mathbf{MLR.3\ No\ Perfect\ Collinearity} &amp; \mathbf{E.2\ No\ Perfect\ Collinearity} \\
None\ of\ x_1,\ x_1,\ \cdots ,\ x_k\ is\ a\ constant\ and\ there\\ are\ no\ exact\ linear\ relationships\ among\ them &amp; \mathbf{X}\ has\ rank\ k+1 \\  
\hline 
\mathbf{MLR.5\ Homoskedasticity}\ &amp; \mathbf{E.4\ Homo\ +\ Randomness} \\ 
Var(u|x_1,\ x_2,\ \cdots ,\ x_k)=\sigma^2 &amp; Var(\mathbf{u}|\mathbf{X})=\sigma^2 \mathbf{I}_n \\ 
\hline
\mathbf{MLR.6\ Normality} &amp; \mathbf{E.5\ Normality}\\ 
Conditional\ on\ \mathbf{X}\ the\ population\ errors&amp;\\ are\ normally\ distributed &amp;\\ 
\hline
\end{array}$$`

---

* Results from the assumptions  
E.1-E.3 Ubiasedness  
E.1-E.4 BLUE
E.1-E.5 T-test, F-test

* ***Unbiasedness***  

To show `\(E(\hat{\beta})=\beta\)`, we need **E.2** (E.2 `\(\Rightarrow\)` `\(\mathbf{X'X}\)` can have a inverse).  
Using **E.1**  `$$\mathbf{\hat{\beta}}=(\mathbf{X'X})^{-1}\mathbf{X'y}=(\mathbf{X'X})^{-1}\mathbf{X'(X\beta+u)}=\mathbf{\beta} + (\mathbf{X'X})^{-1}\mathbf{X'u}$$`  
`$$E(\mathbf{\hat{\beta}})=E[\mathbf{\beta} + (\mathbf{X'X})^{-1}\mathbf{X'u}]=\mathbf{\beta}+E( (\mathbf{X'X})^{-1}\mathbf{X'u}) {\overset{\mathbf{E.3}}{=}} \mathbf{\beta}$$`  
since `\(E(\mathbf{u}|\mathbf{X})=0\Rightarrow E( (\mathbf{X'X})^{-1}\mathbf{X'u})=0\)`

Zero conditional mean is not a problem for _predictive analysis_ because the x set is all we have, we don't want to and won't get causal effect.  


---

However, for _presctiptive analysis_, if we don't controll variables that may be affect by the x we are interested in, we won't get causal effect.  

True model:

`$$wage=\beta_0+\beta_1educ+\beta_2ability +u$$`

Estimate: 

`$$wage=\hat{\alpha}_0+\hat{\alpha}_1educ+\hat{v}$$`

`$$E(\hat{\alpha}_1)=\beta_1+\beta_2\frac{\partial\ ability}{\partial\ educ}\neq\beta_1$$`
"omitted variable bais"  


--

Zero conditional mean  requires  **Strictly exogenous** in time series `\(E(u_t|X_{s1}, X_{s2}, \cdots ,X_{sk})=0\forall s=1,2,\cdots , T\)`  
implication: the error term in any time period t is uncorrelated with each of the regressors in all time periods, past, present, and future.  
`$$Corr(u_t,x_{11})=\cdots=Corr(u_t,x_{T1})=Corr(u_t,x_{1k})=\cdots=Corr(u_t,x_{Tk})=0$$`

It is violated when the regression model contains a lag of the dependent variable as a regressor.

---

* variance-covariance matrix  

For a random variable `\(\mathbf{v}\)` and its expectation 

`$$\mathbf{v}=\left(\begin{array}{c} v_1\\ v_2\\ \vdots\\ v_n \end{array}\right)\qquad E(\mathbf{v})=\underset{n\times 1}{\mu}=\left(\begin{array}{c} \mu_1\\ \mu_2\\ \vdots\\ \mu_n\end{array} \right)$$`  
 
variance `\(\sigma^2_i\)` covariance `\(\sigma^2_{ij}\)`  
`\(Var({\mathbf{A}'\mathbf{v}})=\mathbf{A}'Var({\mathbf{v}})\mathbf{A}\)`  
 
The variance-covariance matrix for `\(\mathbf{v}\)` 
`$$Var(\mathbf{v})=E(\mathbf{v}-\mathbf{\mu})(\mathbf{v}-\mathbf{\mu})'=\left[\begin{array}{cccc} \sigma^2_1 &amp;\sigma_{12} &amp; \cdots &amp; \sigma_{1n} \\ \sigma_{21} &amp; \sigma^2_2 &amp; \cdots &amp; \sigma_{2n}\\ \vdots &amp;\vdots&amp;\ &amp;\vdots\\ \sigma_{n1} &amp; \sigma_{n2} &amp; \cdots &amp; \sigma_n^2\end{array}\right]$$`

---

* The variance of OLS estimator  

`$$Var(\mathbf{\hat{\beta}}|\mathbf{X})=\sigma^2(\mathbf{X'X})^{-1}$$`
Proof: `$$\mathbf{\hat{\beta}}=(\mathbf{X'X})^{-1}\mathbf{X'y}=(\mathbf{X'X})^{-1}\mathbf{X'(X\beta+u)}=\mathbf{\beta} + \underset{\mathbf{A}'}{\underbrace{(\mathbf{X'X})^{-1}\mathbf{X}'}}\mathbf{u}$$`  
`$$\begin{aligned}Var(\mathbf{\hat{\beta}}|\mathbf{X}) &amp;=Var(\mathbf{\beta} +(\mathbf{X'X})^{-1}\mathbf{X'u}|\mathbf{A})\\ &amp;=Var(\mathbf{A}'\mathbf{u}|\mathbf{A})\\ &amp;=\mathbf{A}'Var(\mathbf{u})\mathbf{A}\\ E.5\ \Rightarrow Var(\mathbf{\hat{\beta}})&amp;=\mathbf{A}'\sigma^2\mathbf{I}_n\mathbf{A}\\ &amp;=\sigma^2(\mathbf{X'X})^{-1}\mathbf{X}'\mathbf{X}(\mathbf{X'X})^{-1}\\ &amp;=\sigma^2(\mathbf{X'X})^{-1}\end{aligned}$$`  

---

To estimate `\(\sigma^2\)`, we can use the unbiased estiomator 
`$$\hat{\sigma}^2=\frac{\sum^n_{i=1}\hat{u}^2_i}{n-k-1}=\frac{\hat{\mathbf{u}}'\hat{\mathbf{u}}}{n-k-1}$$`

Think about dimensions: `\(\hat{y}\)` is in the column space of `\(X\)`, so it is in a subspace with dimension `\(k + 1\)`   

`\(\hat{\mathbf{u}}\)` is orthogonal to column space of `\(\mathbf{X}\)`, so it is in a subspace with dimension `\(n-(k+1)=n-k-1\)`. So even though there are n coordinates in `\(\hat{\mathbf{u}}\)`, only `\(n − k − 1\)` of those are free (it has n − k − 1 _degree of freedom_)

--

* *B.L.U.E.*  
   + What is BLUE
      -   Best(smallest variance) Linear Unbiased Estimator  
   + How to prove BLUE  
      -   Using E.1-E.4, based on Gauss-Markov Theorem, `\(\mathbf{\hat{\beta}}\)` is BLUE of `\(\mathbf{\beta}\)`  
      
      
---



* ***Interpretation***

`$$Murder=\beta_0+\beta_1 Assault + \beta_2 Population + u$$`

`$$\widehat{Murder}=\underset{(1.74)}{3.21}+\underset{0.005}{0.044}Assult-\underset{0.027}{0.045}UrbanPop$$`



```
## 
## Call:
## lm(formula = Murder ~ Assault + UrbanPop, data = USArrests)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.5530 -1.7093 -0.3677  1.2284  7.5985 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  3.207153   1.740790   1.842   0.0717 .  
## Assault      0.043910   0.004579   9.590 1.22e-12 ***
## UrbanPop    -0.044510   0.026363  -1.688   0.0980 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.58 on 47 degrees of freedom
## Multiple R-squared:  0.6634,	Adjusted R-squared:  0.6491 
## F-statistic: 46.32 on 2 and 47 DF,  p-value: 7.704e-12
```

---

* Rescaling (e.g. change the unit)  
 + do not create any new information, so it only changes OLS results in predictable and non-substantive ways.  
 
 `$$\begin{aligned} \hat{y}=\hat{\beta_0}+\hat{\beta_1}x_1+\hat{\beta_2}x_2&amp;\Rightarrow \hat{y}=\hat{\beta_0^*}+\hat{\beta_1^*}x_1^*+\hat{\beta_2^*}x_2\\ 1.\quad x_1\rightarrow cx_1&amp;\quad x_1^*=cx_1\\  \Rightarrow \hat{\beta_0^*}&amp;= \hat{\beta_0} \quad \hat{\beta_1^*}= \hat{\beta_1}/c\quad \hat{\beta_2^*}= \hat{\beta_2}\\  2.\quad x_1\rightarrow a+cx_1&amp;\quad x_1^*=a+cx_1\\  \Rightarrow \hat{\beta_0^*}&amp;= \hat{\beta_0}-\hat{\beta_1}/c\quad \hat{\beta_1^*}= \hat{\beta_1}/c\quad \hat{\beta_2^*}= \hat{\beta_2}\\  \end{aligned}$$`

Since `\(\hat{y}\)` does not change residuals, SST, SSE, SSR stay the same, so `\(R^2\)` will not change.

--

`$$\begin{aligned} y=\hat{\beta_0}+\hat{\beta_1}x_1+\hat{\beta_2}x_2+\hat{u}&amp;\Rightarrow y=\hat{\beta_0^*}+\hat{\beta_1^*}x_1^*+\hat{\beta_2^*}x_2+\hat{u}^*\\ y\rightarrow cy\quad y^*&amp;=cy\\ \Rightarrow \hat{\beta_0^*}&amp;= c\hat{\beta_0}\quad \hat{\beta_1^*}= c\hat{\beta_1}\quad \hat{\beta_2^*}= c\hat{\beta_2}\quad \hat{u}^*=c\hat{u}\\ \end{aligned}$$`  

SST, SSE, SSR all change (multiplied by) same amount, so `\(R^2\)` will not change. 


---



###Week 5

`$$E(\hat{\mathbf{\beta}}|\mathbf{X})=\mathbf{\beta}\qquad Var(\hat{\mathbf{\beta}}|\mathbf{X})=\sigma^2 (\mathbf{X'X})^{-1}$$`

`$$\mathbf{u}|\mathbf{X}\sim N(\mathbf{0}, \sigma^2\mathbf{I_n})$$`

* Assumption MLR.6 or E.5 (Normality): Conditional on X the population errors are normally distributed.

`$$Normal\ Distribution\ + Randomness \Rightarrow i.i.d. (independent\ identical\ distribution)$$`

i.e. Conditional on explanatory variables, population errors `\(\mathbf{u}_i\)` are i.i.d. `\(N(0, \sigma^2)\)`

---

* MLR.1 - MLR.6 are **Classical Linear Model (CLM)** assumptions.

Under CLM assumptions 
`$$\hat{\mathbf{\beta}}|\mathbf{X}\sim N(\mathbf{\beta}, \sigma^2(\mathbf{X'X})^{-1})$$`

`$$\hat{\beta}_j|X\sim N(\beta_j, Var(\hat{\beta}_j))$$`

`$$Var(\hat{\beta}_j)=\sigma^2\{(\mathbf{X'X})^{-1}\}_{jj}$$`

`$$\frac{\hat{\beta_j}-\beta_j}{sd(\beta_j)} \sim N(0,1)$$`


`\(sd(\hat{\beta}_j)\)` depends on `\(\sigma\)`, which is unknown.

* Using `\(\hat{\sigma}\)` as an estimator of `\(\sigma\)`, instead of normal distribution, we are getting a **t distribution**.


`$$\frac{\hat{\beta_j}-\beta_j}{se(\beta_j)} \sim t_{n-k-1}=t_{df}$$`

t distribution has fatter tails than N(0,1).  
As df increases, it gets more similar to N(0,1).


---
* ***T TEST***

**t statistic** (or *t ratio*)

`$$t_{\hat{\beta}_j}=\frac{\hat{\beta}_j}{se(\hat{\beta}_j)}$$`

The size or the **significant level** `\(\alpha\)`  
the probability that we reject the null when it is true (*Type I error*)

&gt; If `\(H_0 : \beta_j=r\)`, then `\(\frac{\hat{\beta_j}-r}{se(\beta_j)} \sim t_{n-k-1}\)`

A `\((1-\alpha)\%\)` **confidence interval** is defined as `\(\hat{\beta}_j\pm c\times se(\hat{\beta}_j)\)`, where `\(c\)` is `\((1-\frac{\alpha}{2})\)` percentile of a `\(t_{n-k-1}\)` distribution (Two sided test).

---

**P Value** is the probability that the realization falling out of the range between negative t statistic and positive t statistic. (two sided test)


&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-7-1.png" style="display: block; margin: auto;" /&gt;

---

* **Steps of testing a hypothesis**  

1. Specify the null. e.g. `\(H_0:\beta_1=0\)`   

2. Specify the alternative. e.g. `\(H_1:\beta_1&gt;0\)` (one sided) or `\(H_1:\beta_1\neq0\)` (two sided)   

3. State the test statistic and its distribution under `\(H_0\)`. e.g. `\(\frac{\hat{\beta_j}}{se(\beta_j)} \sim t_{n-k-1}\ \mathbf{under\ H_0}\)`  

4. Specify the level of significance of the test i.e. `\(\alpha=0.05\)`  

5. Find the critical value from the distribution of the test statistic with reference to the alternative hypothesis and the desired significant level, and exercise the rejection rule.  

6. See if the value of `\(t_{calc}\)` test statistic in your sample `\(t_{calc}\)` is inside or outside the rejection zone and express your conclusion with a sensitive sentence   

---

* **Confidence Intervals**

Another way to use classical statistical testing is to construct a confidence interval using the same critical value as was used for a two-sided test.

A `\((1-\alpha)\%\)` confidence intercal is defined as 

`$$\hat{\beta}_j \pm c \times se(\hat{\beta}_j)$$`

where `\(c\)` is the `\((1-\frac{\alpha}{2})\)` percentile of a `\(t_{n-k-1}\)` distribution  

--

The interpretation of a `\((1-\alpha)\%\)` confidence interval is that the interval will cover the true parameter with probability `\((1-\alpha)\)`  

--

If the confidence interval does not contain the value in null hypothesis, we can reject the null hypothesis at the `\(\alpha\%\)` level.  
If the confidence interval does not contain zero, we can deduce that `\(x_j\)` is statistically significant at the `\(\alpha\%\)` level.  

---

* **F-test**

The overall sifnificant: The alternative can only be that at least one of these restriction is not true (i.e. at least one is sifnificant).  

We estimatie two equtions: *the unrestricted model* and *the restricted model*   

`$$F=\frac{(SSR_r-SSR_{ur})/q}{SSR_{ur}/(n-k-1)} \sim F_{q,n-k-1}\ under\ H_0$$`

`\(q\)`: the numerator df (the number of restriction)   `\(n-k-1\)`: the denominator of df 

F-statistic is always postive ( `\(SSR_r&gt;SSR_{ur}\)` )    

--

&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;


---

* **A useful formulation of the F-test**    
  
The SST of the restricted and the unrestricted models are the same.  

`$$SSR_r=(1-R_r^2)SST\qquad SSR_{ur}=(1-R_{ur}^2)SST$$`



`$$F=\frac{(R_{ur}^2-R_r^2)/q}{(1-R_{ur}^2)/(n-k-1)}$$`    

F-test for *overall significant* of a model for the special null hypothesis is that all slop parameter are zero    
`$$F=\frac{R^2/k}{(1-R^2)/(n-k-1)} \sim F_{k,n-k-1}\ under\ H_0$$`   

--

`$$F=\frac{(SSR_r-SSR_{ur})/3}{SSR_{ur}/(n-4)} \sim F_{3,n-4}\ under\ H_0$$`   

`$$H_0:\beta_0=1\ and\ \beta_2=\beta_3=0$$`   

`$$price_i=\beta_0+\beta_1assess_i+\beta_2area_i+\beta_3bed_i+u_i$$`    

`$$price_i=\beta_0+assess_i+u_i$$`    

`$$\Rightarrow price_i-assess_i=\beta_0+u_i$$`    

---

* **Reparameterisation**    
one-sided single restriction, more than one parameter   
`$$y=\beta_0+\beta_1x_1+\beta_2x_2+\beta_3x_3+u$$`

`$$\begin{aligned} H_0: \beta_1 &amp;=\beta_2\\ H_1 : \beta_1 &amp;&gt; \beta_2 \end{aligned}$$`

`$$\text{Define}\ \delta=\beta_1-\beta_2\ \ \ \ \ \hat{\delta}=\hat{\beta_1}-\hat{\beta_2}$$`    

`$$H_0:\delta=0\ \ \ \ H_1:\delta&gt;0$$`     

`$$Var(\hat{\delta})=Var(\hat{\beta_1})+Var(\hat{\beta_2})-2Cov(\hat{\beta_1},\hat{\beta_2})$$`     


Under CLM assumptions `\(\hat{\beta}\)` condotional on x is normally distribution     

`$$\beta_1=\delta+\beta_2$$`  

`$$y=\beta_0+(\delta+\beta_2)x_1+\beta_2x_2+\beta_3x_3+u$$`    

`$$\Rightarrow y=\beta_0+\delta x_1+\beta_2(x_1+x_2)+\beta_3x_3+u$$`    

---



###Week 6


The linear regression model only need to be linear in parameters.
`$$\log{y}=\beta_0+\beta_1x_1+\beta_2x_2+u$$`
`$$y=\beta_0+\beta_1\log{x_1}+\beta_2x_2+u$$`
`$$\log{y}=\beta_0+\beta_1\log{x_1}+\beta_2x_2+u$$`

`$$\begin{array}{cccc}
Model &amp; Dep\ Var &amp; Indep\ Var &amp; Interpretation\\
\hline
level-level &amp; y &amp; x &amp; \Delta y=\beta_1\Delta x\\
level-log &amp; y &amp; \log{x} &amp; \Delta{y} =\frac{\beta_1}{100} (\%\Delta{x})\\ 
log-level &amp; log{y} &amp; x &amp; \%\Delta{y}=100\beta(\Delta{x})\\
log-log &amp; log{y} &amp; \log{x} &amp; \%\Delta{y}=\beta(\%\Delta{x})\\
\end{array}$$`

A strictly positive ranged variable can be logged `\(postively\ \ skewed \xrightarrow{log}less\ \ skewed\)`  
Explanatory variables measured in years are not logged.  
Variables that are already in percentages are not logged.  
If a variable is positively skewed (like income or wealth), taking logarithms makes its distribution less skewed.  

---

* Choose `\(\log{y}\)` or `\(y\)`
 
`$$\log{y}=\widehat{\log{y}}+\widehat{u}$$`
`$$y=e^{\widehat{\log{y}}+\widehat{u}}=e^{\widehat{\log{y}}} \times e^{\widehat{u}}$$`
While `\(\widehat{u}\)` has mean zero, the expected value of `\(e^{\widehat{u}}\)` is not equal to 1 (expectation is applied only up to linear transformation). It is a constant bigger than 1, called `\(\alpha\)`  
i.e. `\(\hat{y}_{from\ log}=\alpha e^{\widehat{\log{y}}}\)`  
To get `\(\alpha\)`, we regress y on `\(e^{\widehat{\log{y}}}\)` with no constant.  
If `\(\alpha&lt;1\)`, we take `\(\alpha=1\)`  



To chose between model of `\(\log{y}\)` and `\(y\)`, we can select log or level model based on which models prediction has a higher corrletion with `\(y\)` (the true value in the sample).

i.e.
`$$\widehat{Corr}(y, \hat{y})\ \ \ \ v.s.\ \ \ \ \widehat{Corr}(y, \hat{y}_{from\ log})$$`

---

* Quadratic terms 

`$$\widehat{Murder}=3.207+0.0439 Assault -0.0445 Population$$`

`$$\widehat{Murder}=1.514+0.07835 Assault -0.000094 {Assault}^2 -0.0568 Population$$`




`$$Murder=\beta_0+\beta_1 Assault + \beta_2 {Assault}^2 +\beta_3 Population + u$$`  

`$$\frac{\Delta{Murder}}{\Delta{Assault}}=\beta_1 +2\beta_2 Assault$$`
dependes on the number of assault alredy happened.  

&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-9-1.png" style="display: block; margin: auto;" /&gt;

--

```
Maximum or minimum?
```
---

* **Transforming non-stationary series to stationary seies**

**Simple return** : from time `\(t-1\)` to `\(t\)` 

Simple gross return 

`$$1+ R_t =\frac{P_t}{P_{t-1}}$$` 

Simple net return 

`$$R_t =\frac{P_t}{P_{t-1}}-1=\frac{P_t-P_{t-1}}{P_{t-1}}$$`

--

**Log return** : The natural logarithm of the simple gross return is called the log return

`$$r_t=\ln{(1+R_t)}=\ln{\frac{P_t}{P_{t-1}}}=\ln{P_t}-\ln{P_{t-1}}$$`

For small `\(R_t\)` , `\(r_t=\ln{(1+R_t)} \approx R_t\)`

---

&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-10-1.png" width="45%" height="60%" /&gt;&lt;img src="ETC2410_slides_S1_2019_files/figure-html/unnamed-chunk-10-2.png" width="45%" height="60%" /&gt;

---

* Prescriptive analytics  

  + Investigating **causal effect** of an x on a y, while adding all other possible variables as control variables.   
  + Inferences of all those other variables are minor objectives.

--

* Predictive analytics  
  + **Not** investigating causal effect.  
  + Simply trying to predic y based on some xs while keeping the model **simple**.  
  + *Parsimony*   ***KISS Principle***: Keep it simple, stupid.  

---

* **Model Selection Criteria**

General Form

`$$\begin{array}{ccc}I(k)=c\ \ \ +&amp;\ln{[SSR(k)]}\ \ \ +&amp;(k+1)\frac{\alpha}{T}\\  \  &amp; decreasing\ in\ k&amp;increasing \ in\ k\\  \end{array}$$`  

`$$\begin{array}{l}1.\quad Adjusted\ R^2\ \ (\bar{R^2}) \\  \bar{R^2}= 1-\frac{{SSR}/{n-k-1}}{{SST}/{n-1}}\\  \hline 2.\quad Akaike Information Caiteria\ \ (AIC) \\  AIC=c_1 +\ln(SSR) + \frac{2k}{n}\\ \hline 3.\quad Hannan-Quinn Criterion\ \ (HQ)\\  HQ=c_2 +\ln(SSR) + \frac{2k\ln({\ln{(n)}})}{n}\\ \hline 4.\quad Schwarz\ or\ Bayesian\ Information\ Criterion\ \ (SIC\ or\ BIC)\\ BIC=c_3+\ln(SSR) + \frac{k\ln{(n)}}{n}\\  \end{array}$$`   


order of penalties
`$$P(BIC) &gt; P(HQ) &gt;P(AIC)&gt;P(\bar{R^2})$$`

---

* **Predictions and Prediction Intervals**

**Two Sources of Error**   
 - 1.Estimation uncertainty: caused by not knowing the value of the true parameters    
 
 - 2.u: not predictable by our predictor, even if we know the true value of `\(\beta\)`    



* std.error `\(\rightarrow\)` Estimation uncertainty

`$$\widehat{Murder}=6.41594+0.02093Assault$$`

`$$\widehat{Murder}=\beta_0+\beta_1(Assault-170)$$`


---

The relative magnitudes of `\(Var(u)\)` and `\(Var(\hat{y})\)` is 1 to `\(1/n\)`, so often ignore estimation uncertainty.  

Only consider estimation uncertainty, 95% confidence interval for `\(E(y_i|x_{i1}, \cdots,x_{ik})\)` : `$$\hat{y}\pm (cv(t_{n-k-1}, two\ tailed:0.05)\times se(\hat{y})$$`  

--

95% prediction interval for `\(y_i\)` :
`$$\hat{y}\pm (cv(t_{n-k-1}, two\ tailed:0.05)\times se(\hat{e})$$`   
where
`$$se(\hat{e})=\sqrt{\hat{\sigma}^2+[se(\hat{y})]^2}$$`

--

Ignoring estimation uncertainty, 95% prediction interval for `\(y_i\)` :
`$$\hat{y}\pm (cv(t_{n-k-1}, two\ tailed:0.05)\times \hat{\sigma}$$`   

---

`$$\widehat{Murder}=6.41594+0.02093Assault$$`




```
## 
## Call:
## lm(formula = Murder ~ Assault, data = USArrests)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8528 -1.7456 -0.3979  1.3044  7.9256 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 0.631683   0.854776   0.739    0.464    
## Assault     0.041909   0.004507   9.298  2.6e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.629 on 48 degrees of freedom
## Multiple R-squared:  0.643,	Adjusted R-squared:  0.6356 
## F-statistic: 86.45 on 1 and 48 DF,  p-value: 2.596e-12
```

---

`$$\widehat{Murder}=\beta_0+\beta_1(Assault-1000,000)$$`

```
## 
## Call:
## lm(formula = Murder ~ I(Assault - 170), data = USArrests)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8528 -1.7456 -0.3979  1.3044  7.9256 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      7.756149   0.371863  20.858  &lt; 2e-16 ***
## I(Assault - 170) 0.041909   0.004507   9.298  2.6e-12 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.629 on 48 degrees of freedom
## Multiple R-squared:  0.643,	Adjusted R-squared:  0.6356 
## F-statistic: 86.45 on 1 and 48 DF,  p-value: 2.596e-12
```

---
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"includes": {
"in_header": "mylatexpackage.sty"
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
