<!DOCTYPE html>
<html>
  <head>
    <title>ETC2410</title>
    <meta charset="utf-8">
    <meta name="author" content="Fin" />
    <link href="2018S1-2017S2_files/remark-css-0.0.1/default.css" rel="stylesheet" />
    <link href="2018S1-2017S2_files/remark-css-0.0.1/default-fonts.css" rel="stylesheet" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# ETC2410
### Fin
### Semester 2 2017

---






 


* **Trend**  

 - **Positive time trend** : A time series which displays a strong tendency to *increase* over time.
 
 - **Negative time trend** : A time series which displays a strong tendency to *decrease* over time.

Time trends in economic time series are generally attributable to underlying economic or demographic factors affecting the time series.  

 

* **Adding a deterministic function of `\(t\)` in the model**  


`$$y_t=\alpha_0 + \alpha_1 t + e_t,\ \ t=1, 2, \cdots , T, \text{where}\ \ e_t\sim i.i.d.(0, \sigma^2)$$`  

`\(\alpha_1 t\)` is called a **linear time trend** (a linear function of the variable t), and a **deterministic trend** (as opposed to a stochastic trend) (the value of t is known in each time period in advance of collecting any data)  

---



`$$y_t=\alpha_0 + \alpha_1 t + e_t,\ \ t=1, 2, \cdots , T, \text{where}\ \ e_t\sim i.i.d.(0, \sigma^2)$$`  

`$$E(y_t)=\alpha_0+\alpha_1t$$`  

`$$\begin{aligned} \Delta E(y_t) &amp;= E(y_t)-E(y_{t-1})\\ &amp;= [\alpha_0 +\alpha_0 t] -[\alpha_0 +\alpha_1(t-1)]\\ &amp;= \alpha_0 +\alpha_1 t-\alpha_0 -\alpha_1t +\alpha_1\\ &amp;=\alpha_1 \end{aligned}$$`

Parameter `\(\alpha_1\)` measures the change in the expected or average value of the time series from one time period to the next



`$$\alpha_1 &gt; 0 \Rightarrow E(y_t) \text{  is increasing over time}$$`  

`$$\alpha_1 &lt; 0 \Rightarrow E(y_t) \text{  is decreasing over time}$$`  

---



---
* **autocorrelation function (ACF)**

The correlation coefficient `\(\rho_j\)` captures both the direct and indirect effect on `\(y_t\)` of a change in `\(y_{T-J}\)` .  

* **partial autocorrelation function (PACF)**

The partial correlation coefficient captures the direct effect only.  




---
    
* **Autocorrelation Function (ACF)**

`$$\begin{aligned} \rho_j &amp;= \frac{cov(y_t, y_{t-j})}{\sqrt{Var(y_t)}\sqrt{Var(y_{t-j})}}\\ &amp;= \frac{\gamma_j}{\sqrt{\gamma_0}\sqrt{\gamma_0}}=\frac{\gamma_j}{ \gamma_0}\end{aligned}$$`
    
under the stationarity assumption `\(Var(y_t)=Var(y_{t-j})=\gamma_0\)`

`$$\begin{aligned} \rho_{-j} &amp;= \frac{cov(y_t, y_{t+j})}{\sqrt{Var(y_t)}\sqrt{Var(y_{t+j})}}\\ &amp;=\frac{\gamma_{-j}}{ \gamma_0}\end{aligned}$$`

Under the stationarity assumption 

`$$\rho_{-j}=\rho_j$$`

---

The parameter `\(\rho_j\)` measures the strength of the linear relationship between `\(y_t\)` and `\(y_{t-j}\)` 

* **Properties**

&gt; P1 `\(\rho_0 =\frac{\gamma_0}{\gamma_0}=1\)`    
  P2 `\(-1\leq \rho_j \leq 1 \ , \ \forall j\)`  
  P3 The sign of `\(\rho_j\)` indicates the direction of the linear relationship between `\(y_t\)` and `\(y_{t-j}\)` 
  
* **features**  

&gt; (1) `\(\rho_j\)` is unit free  
  (2) `\(\rho_j\)` is easy to interpret  
  
---

* **Partial Autocorrelation Function (PACF)**  

`\(\varphi_{jj}\)` : The partial autocorrelation at lag `\(j\)` , which we denote by `\(\varphi_{jj}\)` , measures the correlation between `\(y_t\)` and `\(y_{t-j}\)` , when the intermediate values `\(y_{t-1}, \cdots , y_{t-j+1}\)` are held constant.  

e.g. `$$\varphi_{22}\ :\ y_t=\varphi_{21}y_{t-1}+\varphi_{22}y_{t-2}+u_t$$`  

A change in `\(y_{t-2}\)` holding `\(y_{t-1}\)` chonstant has a **direct effect** on `\(y_t\)` , captured by `\(\varphi_{22}\)` : `\(\varphi_{22}=\frac{\partial y_t}{\partial y_{t-2}}\)`

(where `\(y_{t-1}=\varphi_{21}y_{t-2}+\varphi_{22}y_{t-3}+u_{t-1}\)` representing the **indirect effect** on `\(y_t\)` ) 

The PACF is obtained by plotting `\(\varphi_{jj}\)` against `\(j\)` .  
The SPACF is obtained by plotting `\(\hat{\varphi}_{jj}\)` against non-negative value of `\(j\)` .  

---

`$$\begin{array}{c|c} \textbf{Population parameters} &amp; \textbf{Sample statistics}\\ 
E(y_t)=\mu=\int^{\infty}_{-\infty}y_tf(y_t)dy &amp; \bar{y}=\frac{1}{T}\sum^T_{t=1}y_t\\ 
\gamma_0=E[(y_t-\mu)^2] &amp; \hat{\gamma}_0=\frac{1}{T}\sum^T_{t=1}(y_t-\bar{y})^2\\  \gamma_j=E[(y_t-\mu)(y_{t-j}-\mu)] &amp; \hat{\gamma}_j=\frac{1}{T}\sum^T_{t=1}(y_t-\bar{y})(y_{t-j}-\bar{y})\\ 
\rho_{j} = \frac{cov(y_t, y_{t-j})}{\sqrt{Var(y_t)}\sqrt{Var(y_{t-j})}} =\frac{\gamma_{j}}{ \gamma_0} &amp; \hat{\rho}_{j}=\frac{\hat{\gamma_{j}}}{\hat{\gamma_0}}= \frac{\sum^T_{t=j+1}(y_t-\bar{y})(y_{t-j}-\bar{y})}{\sum^T_{t=j+1}(y_t-\bar{y})^2}
\end{array}$$`

The observed data are being generated by some underlying statistical model -- *data generating process* (DGP)  
Strategy:  
1. construct SACF, SPACF  
2. identify DGP  
3. estimate the selected model by OLS  
4. forecast

---

* **Testing for autocorrelation - The Ljung-Box Q test**  

`$$\begin{aligned}
H_0 &amp;: \rho_1=\rho_2=\rho_3=\cdots =\rho_s=0\\ 
H_1 &amp;: \rho_j\neq 0 \text{ for at least one } j=1,2,\cdots , s
\end{aligned}$$`

$$\text{Test statistic  } Q_s=T(T+2)\sum^s_{j=1}\frac{\hat{\rho}^2_j}{(T-j)} $$

`$$\text{Under } H_0\ \ Q_s \overset{asy}{\sim} \chi^2 (s)$$` 

(may be unreliable in small samples)

* **Test autocorrelation in the error term**  

`$$\text{Under } H_0\ \ Q_s \overset{asy}{\sim} \chi^2 (s-p)$$`  

the first lag for which the test statistic is available is s=p+1

---

###Week 9

---

* **The AR(P) model**    

`$$y_t=\varphi_0 + \varphi_1y_{t-1} + \varphi_2y_{t-2} + \cdots + \varphi_py_{t-p} + u_t$$`      

&gt; The ACF declines exponentially    
  The PACF cuts off after p lags or `\(\varphi_{jj}=0,\ \forall \geq p\)`     

These observations suggest that if the ACF of `\(\{y_t\}\)` declines exponentially, and the PACF cuts off after lag p, then an AR(p) model might be a good model to represent the DGP of `\(\{y_t\}\)`      

---

`\(p\)` : known order for an autoregressive process, **lag truncation parameter** , is actually unknown and need to be chosen.

* **Method for lag selection**   

&gt; M1 Inspection of the SACF and SPACF  

* if the SACF declines exponentially and the sample autocorrelation coefficients become statistically insignificant after p lags, this suggests that the order of the autoregression is p.

&gt; M2 The general to specific rule  

* t test the last lag parameter, repeat the process.  
 - initial choice of p is arbitrary    
 - the choice of p will be too large, at least some of the time (5% chance fail to reject)  
 - in real world, for highly correlated variables, near multinearity problem (s.d. increasing, t value decreasing, wrong test)  

&gt; M3 Information criteria     


---

* **Autoregressive Distributed lag Models (ADL model)**  

`$$\begin{aligned}ADL(p,q):\ \ y_t=c&amp;+\varphi_1 y_{t-1}+\varphi_2y_{t-2}+\cdots +\varphi_py_{t-p}\\ &amp;\ +\alpha_1x_{t-1}+\alpha_2x_{t-2}+\cdots +\alpha_q x_{t-q}+u_t
\end{aligned}$$`

‘*autoregressive*’ because lagged values of the dependent variable are included as regressors.  
‘*distributed lag*’ because lags of an additional regressor, in this case INF, are also included in the model.

---

* **Model checking**  

We need to check if the model is adequate. (if it is, resideal should be *white noise*)

If not `\(\Rightarrow\)`  
If some fitted coefficients are very small, the model could be simplified by removing these coefficients.    
If residuals show correlation then the model should be extended to take care of those correlations.  


---

###Week 10

* **Forecasting stationary autoregressive time series**  
at time T (*forecast origin*), seeing time T+k (k:*forecast horizon*)  

 - **point forecast** for `\(y_{T+k}\)` : this implies that `\(y_{T+k}\)` will assume a particular value.  
 
 - **interval forecast** (or **forecast interval**): this implies that `\(y_{T+k}\)` will lie in a specified interval with a specified probability.  
 
Forecast intervals are generally more useful than point forecasts since they assess how con…dent we are about the accuracy of our forecast.

* with a sample of `\(T\)` observations `\((y_1, y_2, \cdots , y_T)\)`  

 - The **prediction** is `\(\hat{y}_t=\hat{c} +\hat{\varphi}_1y_{t-1}, \ \ \ t=2, 3, \cdots , T\ \ \Rightarrow\)` within-sample observations  
 - A **forecast** is `\(y_{T+j}, \text{for }\ j\geq 1\ \ \ \Rightarrow\)` out-of-sample observations  
 
---

* **Forecast error**  

`\(F_{T+k|T}\)` denotes the forecast value of `\(y_{T+k}\)` in period `\(T\)` , given that we observe `\((y_T, y_{T-1}, \cdots , y_1)\)`  

The k step ahead forecast error 

`$$FE(k)=y_{T+k}-F_{T+k|T}$$` 

the mean of the k-steo ahead forecast error 

`$$E(FE(k))=E(y_{T+k}-F_{T+k|T})$$`

* **Forecasting criteria**  

 - Specifies how forecasts are to be formed  
 - Uses only currently available information  
 - Leads to forecasts which are in some sense 'accurate'  
 
---

* **Forecasting criteria**  

&gt; Choose `\(F_{T+k|T}\)` to minimize  
`$$E(FE(k)^2)=E((y_{T+k}-F_{T+k|T})^2)$$`

which is the mean squared forecast error MSFE(k), or mean squared error of the forecast MSE(k)  

&gt; by setting 
`$$F_{T+k|T}=E_T(y_{T+k})$$` 

which is the forecast of `\(y_{T+k}\)` which minimizes the mean squared error of the forecast.  

&gt; In practice, `\(F_{T+k|T}=\hat{E}_T(y_{T+k})\)`

---

* **Point forecasts**  

For AR(1)
`$$y_t=c+\varphi_1y_{t-1}+u_t\ \ \text{where} \ \ E_{t-1}(u_t)=E(u_t|y_{t-1}, y_{t-2}, \cdots , y_1)$$`

Steps for `\(y_{T+1}\)`  

&gt; **S1** Updating to  
`$$y_{T+1}=c+\varphi_1y_T+u_{T+1}$$`  

&gt; **S2** Taking conditional expections  
`$$E_T(y_{T+1})=E_T(c+\varphi_1y_T+u_{T+1})=c + \varphi_1E_T(y_T)+E_T(u_{T+1})=c+\varphi_1y_T$$`
since `\(E_T(y_T)=y_T\)` , `\(E_T(u_{T+1})=0\)`  

&gt; **S3** Replacing the unknown parameters  
`$$F_{T+1|T}=\hat{E}_T(y_{T+1})=\hat{c}+\hat{\varphi}_1y_T$$`  

To forcast `\(y_{T+2}\)` , we repeat S1 - S3 
`$$E_T(y_{T+2})=c+\varphi_1(c+\varphi_1y_t)=(1+\varphi_1)c+\varphi_1^2y_T$$`

---

* The forecasts of `\(\{y_t\}\)` are generated **recursively**  

`$$F_{T+1|T}=\hat{c}+\hat{\varphi}_1y_T$$`

`$$F_{T+2|T}=\hat{c}+\hat{\varphi}_1F_{T+1|T}$$`

`$$F_{T+3|T}=\hat{c}+\hat{\varphi}_1F_{T+2|T}$$`

--- 

`$$y_{T+1}=c+\varphi_1y_T+u_{T+1}\quad \text{and}\quad F_{T+1|T}=\hat{c}+\hat{\varphi}_1y_T$$`  

`$$FE(1)=y_{T+1}-F_{T+1|T}=(c+\varphi_1y_T+u_{T+1})-(\hat{c}+\hat{\varphi}_1 y_T)\\ =u_{T+1}+(c-\hat{c})+(\varphi_1-\hat{\varphi}_1)y_T$$`

---

* **Intrinsic uncertainty**  

`\(u_{T+1}\neq E_T(u_{T+1})=0\)` the future is intrinsically uncertain.

Not predictable by our predictors, even if we know the true value of parameters.  

* **Estimation uncertainty**  

We make errors when we estimate the unknown parameters of the model.

`\(\hat{c}\neq c\)` and `\(\hat{\varphi}_1\neq \varphi_1\)` , caused by not knowing the valur of the true parameters  
---

A forecast interval is like a confidence interval except a forecast interval pertains to the future value of a time series and confidence interval pertains to an unknown parameter.  

* **Interval Forecasts**  

For AR(1) 
`$$FE(1)=u_{T+1}+(c-\hat{c})+(\varphi_1-\hat{\varphi}_1)y_T$$`  
with the assumption: `\(u_t\sim i.i.d.N(0,\sigma^2)\)`  

`$$FE(1)\sim N(0, FEV(1))$$`
where FEV is the one-step ahead forecast error variance  

`$$\frac{FE(1)}{sd(1)}\sim N(0,1)$$`
where `\(sd(1)=\sqrt{FEV(1)}\)` is the standard deviation of FE(1)  

---

`$$P[-1.96\times sd(1)&lt;FE(1)&lt;1.96\times sd(1)]=0.95$$` 
where `\(FE(1)=y_{T+1}-F_{T+1|T}\)`

`$$P[F_{T+1|T}-1.96\times sd(1)&lt;y_{T+1}&lt;F_{T+1|T}+1.96\times sd(1)]=0.95$$` 

`$$P[F_{T+1|T}-1.96\times SEF(1)&lt;y_{T+1}&lt;F_{T+1|T}+1.96\times SEF(1)]=0.95$$` 
where `\(SEF(1)\)` is an estimator of `\(sd(1)\)` and is known as the standard error of the forecast (standard error of the one-step ahead forecast)

For AR(p)

`$$F_{T+1|T}=\hat{E}_T(Y_{T+1})=\hat{c}+\hat{\varphi}_1y_T+\hat{\varphi}_2y_{T-1}+\cdots +\hat{\varphi}_py_{T-p+1}$$`

`$$F_{T+2|T}=\hat{E}_T(Y_{T+2})=\hat{c}+\hat{\varphi}_1F_{T+1|T}+\hat{\varphi}_2y_{T}+\cdots +\hat{\varphi}_py_{T-p+2}$$`

`$$P[F_{T+k|T}-1.96\times SEF(k)&lt;y_{T+k}&lt;F_{T+k|T}+1.96\times SEF(k)]=0.95$$`  

The practice of estimating a model over one time period to generate forecasts for a different time period is called **out-of-sample forecasting**.   

---

* **Forecast evaluation formulae**

Root mean squared error  

`$$RMSE=\sqrt{\frac{\sum^{T+k}_{t=T+1}(y_t-y_t^f)^2}{k}}$$`  

Mean absolute error  

`$$MAE=\sum^{T+k}_{t=T+1}\frac{1}{k}\left|y_t-y_t^f\right|$$`

Mean absolute percentage error  

`$$MAPE=100\sum^{T+k}_{t=T+1}\frac{1}{k}\left|\frac{y_t-y_t^f}{y_t}\right|$$`

where  
`\(y_t\)` = actual value of the time series in period t  
`\(y_t^f\)` = forecast value of the time series in period t

---

---

* **Consistent estimator**  

`\(\hat{\beta}_T\)` is a consistent estimator of `\(\beta\)` if there is a high probability that `\(\hat{\beta}_j\)` is "very close" to `\(\beta\)` in a large sample.  

`$$p\lim({\hat{\beta}_T})=\beta$$`  

`$$Pr(|\hat{\beta}_T-\beta|&lt;\epsilon)\rightarrow1\ \text{as}\ \ T\rightarrow \infty \text{ for all }\epsilon &gt;0$$`  

`\(\hat{\beta}_T\)` converges in probability to `\(\beta\)` , or, `\(\beta\)` is the probability limit of `\(\hat{\beta}_T\)`.  

* **Theorm(5)**  
The OLS estimator of `\(\beta\)` in the linear regression model `\(\mathbf{y}_t=\mathbf{X_t'\beta}+\mathbf{u_t}, \ \ t=1, 2, \cdots, T\)` si a consistent estimator of `\(\beta\)` when  
&gt; **D1** The stochastic process `\(\{y_t, x_{t1}, \cdots , x_{tk}:t=1, 2, \cdots \}\)` is stationary and weakly dependent  
&gt; **D2** The regressors `\((x_{t1}, \cdots , x_{tk})\)` are contemporaneously exogenous  

---


* A stationary time series `\(\{x_t\}\)` is **weakly dependent** if `\(Corr(x_t, x_{t+h})\)` goes to zero "sufficiently quickly" as h tends to infinity.  

**Violation of consistency**: lag of y in regressors  
`$$\begin{aligned} y_t&amp;=\beta_0 +\beta_1y_{t-1}+u_t\\  y_{t-1}&amp;=\beta_0+\beta_1y_{t-2}+u_{t-1}\\ 
corr&amp;(y_{t-1}, u_{t-1})\neq 0 \\ \text{and if}\ \ corr&amp;(u_t, u_{t-1})\neq0\ \ \text{(first order autocorrelation)}\\ \text{ then }\ corr&amp;(y_{t-1}, u_{t})\neq 0 \end{aligned}$$`

---

* **Random walk with a drift versus trend stationary time series**  

For a series with a deterministic time trend  
`$$y_t=\alpha_0+\alpha_1t+e_t\ \text{where} \ e_t\sim IID(0,\sigma^2)$$`  

* we have that:  
    1. `\(y_t\)` grows linearly with time  
    2. the mean `\(E(y_t)=\alpha_0+\alpha_1t\)` is time dependent  
    3. the variance `\(Var(y_t)=E[y_t-E(y_t)]^2=E(e^2_t)=\sigma^2\)` is constant  
    
A unit root with a drift model has a variance that is time dependent.
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"includes": {
"in_header": "mylatexpackage.sty"
}
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
